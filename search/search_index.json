{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#what-is-it","title":"What is it?","text":"<p>mistral-common is a set of tools to help you work with Mistral models.</p> <p>We open-source the tokenizers, validation and normalization code that can be used with our models.</p> <p>This ensures that you can take full advantage of our models for the following features:</p> <ul> <li>tokenization of text, images and tools calls.</li> <li>validation and normalization of requests, messages, tool calls, and responses. This is built on top of the Pydantic library.</li> </ul> <p>We also version our tokenizers to guarantee backward compatibility for the models that we release.</p>"},{"location":"#for-who","title":"For who ?","text":"<p>This library is for you if you want to:</p> <ul> <li>use our models in your own application.</li> <li>build your own models and want to use the same tokenization and validation code as we do.</li> </ul> <p>See the complete list of models that we released and the tokenizers that you can use with them.</p>"},{"location":"#table-of-contents","title":"Table of contents","text":"<p>Explore the following sections:</p> <ul> <li>Usage section to install and use the library.</li> <li>Examples section to see how to use the library with our models.</li> <li>Models section to see the list of models that we released and the tokenizers that you can use with them.</li> <li>Code Reference section to see the code documentation.</li> </ul>"},{"location":"#launch-the-documentation-locally","title":"Launch the documentation locally","text":"<p>To launch the documentation locally, simply run the following command at the root of the repository: <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"models/","title":"Mistral Models","text":"<p>At Mistral, we believe in the power of openness and broad distribution to promote innovation and collaboration in AI. Based on this principle, we release models to the community that are open for anyone to use, subject to the terms of the license. These models supports wide range of use cases:</p> <ul> <li>Text generation and chat</li> <li>Code generation and editing</li> <li>Math reasoning</li> <li>Multimodal generation and chat</li> <li>Coding agent.</li> </ul> <p>To learn more about all of our models, please visit our website.</p>"},{"location":"models/#list-of-open-models","title":"List of Open models","text":"Model Task Text Image Tokenizer Context size License HF Hub Repo Mistral 7B v0.1 Generation \u2705 \u274c v1 8k Apache 2.0 <code>mistralai/Mistral-7B-v0.1</code> Mistral 7B Instruct v0.1 Instruct \u2705 \u274c v1 8k Apache 2.0 <code>mistralai/Mistral-7B-Instruct-v0.1</code> Mixtral 8x7B Generation \u2705 \u274c v1 32k Apache 2.0 <code>mistralai/Mixtral-8x7B-v0.1</code> Mixtral 8x7B Instruct v0.1 Instruct \u2705 \u274c v1 32k Apache 2.0 <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> Mistral 7B Instruct v0.2 Instruct \u2705 \u274c v1 32k Apache 2.0 <code>mistralai/Mistral-7B-Instruct-v0.2</code> Mixtral 8x22B v0.1 Generation \u2705 \u274c v1 65k Apache 2.0 <code>mistralai/Mixtral-8x22B-v0.1</code> Mixtral 8x22B Instruct v0.1 Instruct \u2705 \u274c v1 65k Apache 2.0 <code>mistralai/Mixtral-8x22B-Instruct-v0.1</code> Mistral 7B v0.3 Generation \u2705 \u274c v3 32k Apache 2.0 <code>mistralai/Mistral-7B-v0.3</code> Mistral 7B v0.3 Instruct Instruct \u2705 \u274c v3 32k Apache 2.0 <code>mistralai/Mistral-7B-Instruct-v0.3</code> Codestral 22B v0.1 Instruct and FIM \u2705 \u274c v3 32k MNPL <code>mistralai/Codestral-22B-v0.1</code> Mathstral 7B v0.1 Instruct \u2705 \u274c v3 32k Apache 2.0 <code>mistralai/Mathstral-7B-v0.1</code> Mamba Codestral 7B v0.1 Instruct and FIM \u2705 \u274c v3 256k Apache 2.0 <code>mistralai/Mamba-Codestral-7B-v0.1</code> Mistral Nemo Base 2407 Generation \u2705 \u274c v3 - tekken 128k Apache 2.0 <code>mistralai/Mistral-Nemo-Base-2407</code> Mistral Nemo Instruct 2407 Instruct \u2705 \u274c v3 - tekken 128k Apache 2.0 <code>mistralai/Mistral-Nemo-Instruct-2407</code> Mistral Large Instruct 2407 Instruct \u2705 \u274c v3 - tekken 128k MRL <code>mistralai/Mistral-Large-Instruct-2407</code> Pixtral 12B Base 2409 Generation \u2705 \u2705 v3 - tekken 128k Apache 2.0 <code>mistralai/Pixtral-12B-Base-2409</code> Pixtral 12B 2409 Instruct \u2705 \u2705 v3 - tekken 128k Apache 2.0 <code>mistralai/Pixtral-12B-2409</code> Mistral Large Instruct 2411 Instruct \u2705 \u274c v7 128k MRL <code>mistralai/Mistral-Large-Instruct-2411</code> Pixtral Large Instruct 2411 Instruct \u2705 \u2705 v7 128k MRL <code>mistralai/Pixtral-Large-Instruct-2411</code> Mistral Small 24B Base 2501 Generation \u2705 \u274c v7-tekken 32k Apache 2.0 <code>mistralai/Mistral-Small-24B-Base-2501</code> Mistral Small 24B Instruct 2501 Instruct \u2705 \u274c v7-tekken 32k Apache 2.0 <code>mistralai/Mistral-Small-24B-Instruct-2501</code> Mistral Small 3.1 24B Base 2503 Generation \u2705 \u2705 v7-tekken 128k Apache 2.0 <code>mistralai/Mistral-Small-3.1-24B-Base-2503</code> Mistral Small 3.1 24B Instruct 2503 Instruct \u2705 \u2705 v7-tekken 128k Apache 2.0 <code>mistralai/Mistral-Small-3.1-24B-Instruct-2503</code> Devstral Small 2505 Coding Agent \u2705 \u274c v7-tekken 128k Apache 2.0 <code>mistralai/Devstral-Small-2505</code>"},{"location":"code_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mistral_common<ul> <li>base</li> <li>exceptions</li> <li>image</li> <li>multimodal</li> <li>protocol<ul> <li>base</li> <li>embedding<ul> <li>request</li> <li>response</li> </ul> </li> <li>instruct<ul> <li>converters</li> <li>messages</li> <li>normalize</li> <li>request</li> <li>response</li> <li>tool_calls</li> <li>validator</li> </ul> </li> <li>utils</li> </ul> </li> <li>tokens<ul> <li>instruct<ul> <li>request</li> </ul> </li> <li>tokenizers<ul> <li>base</li> <li>image</li> <li>instruct</li> <li>mistral</li> <li>multimodal</li> <li>sentencepiece</li> <li>tekken</li> <li>utils</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"code_reference/mistral_common/base/","title":"base","text":""},{"location":"code_reference/mistral_common/base/#mistral_common.base","title":"<code>mistral_common.base</code>","text":""},{"location":"code_reference/mistral_common/base/#mistral_common.base.MistralBase","title":"<code>MistralBase(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all Mistral Pydantic models.</p> <p>Forbids extra attributes, validates default values and use enum values.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/","title":"exceptions","text":""},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions","title":"<code>mistral_common.exceptions</code>","text":""},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidAssistantMessageException","title":"<code>InvalidAssistantMessageException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid assistant messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidAssistantMessageException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidFunctionCallException","title":"<code>InvalidFunctionCallException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid function calls.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidFunctionCallException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidMessageStructureException","title":"<code>InvalidMessageStructureException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid message structures.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidMessageStructureException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidRequestException","title":"<code>InvalidRequestException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid requests.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidRequestException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidSystemPromptException","title":"<code>InvalidSystemPromptException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid system prompts.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidSystemPromptException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidToolException","title":"<code>InvalidToolException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid tools.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidToolException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidToolMessageException","title":"<code>InvalidToolMessageException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid tool messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidToolMessageException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidToolSchemaException","title":"<code>InvalidToolSchemaException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid tool schemas.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidToolSchemaException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidUserMessageException","title":"<code>InvalidUserMessageException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid user messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidUserMessageException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.MistralCommonException","title":"<code>MistralCommonException(message=None)</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all Mistral exceptions.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> <p>If no message is provided, the default message is used.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str]</code> <p>A human-readable message describing the error.</p> <code>None</code> Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: Optional[str] = None,\n) -&gt; None:\n    r\"\"\"Initialize the `MistralCommonException` with an optional message.\n\n    If no message is provided, the default message is used.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    if message:\n        self.message = message\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.TokenizerException","title":"<code>TokenizerException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for errors in the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `TokenizerException` with a message.\n\n    Args:\n      message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.UnsupportedTokenizerFeatureException","title":"<code>UnsupportedTokenizerFeatureException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for unsupported features in the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `UnsupportedTokenizerFeatureException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/image/","title":"image","text":""},{"location":"code_reference/mistral_common/image/#mistral_common.image","title":"<code>mistral_common.image</code>","text":""},{"location":"code_reference/mistral_common/image/#mistral_common.image.download_image","title":"<code>download_image(url)</code>","text":"<p>Download an image from a URL and return it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image to download.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The downloaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def download_image(url: str) -&gt; Image.Image:\n    r\"\"\"Download an image from a URL and return it as a PIL Image.\n\n    Args:\n        url: The URL of the image to download.\n\n    Returns:\n       The downloaded image as a PIL Image object.\n    \"\"\"\n    headers = {\"User-Agent\": f\"mistral-common/{__version__}\"}\n    try:\n        # Make a request to download the image\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n\n        # Convert the image content to a PIL Image\n        img = Image.open(io.BytesIO(response.content))\n        return img\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error downloading the image from {url}: {e}.\")\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to PIL image: {e}\")\n</code></pre>"},{"location":"code_reference/mistral_common/image/#mistral_common.image.maybe_load_image_from_str_or_bytes","title":"<code>maybe_load_image_from_str_or_bytes(x)</code>","text":"<p>Load an image from a string or bytes.</p> <p>If the input is already a PIL Image, return it as is.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Image, str, bytes]</code> <p>The input to load the image from. Can be a PIL Image, a string, or bytes. If it's a string, it's assumed to be a base64 encoded string of bytes.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The loaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def maybe_load_image_from_str_or_bytes(x: Union[Image.Image, str, bytes]) -&gt; Image.Image:\n    r\"\"\"Load an image from a string or bytes.\n\n    If the input is already a PIL Image, return it as is.\n\n    Args:\n        x: The input to load the image from. Can be a PIL Image, a string, or bytes.\n            If it's a string, it's assumed to be a base64 encoded string of bytes.\n\n    Returns:\n       The loaded image as a PIL Image object.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x\n    if isinstance(x, bytes):\n        try:\n            return Image.open(io.BytesIO(x))\n        except Exception:\n            raise RuntimeError(\"Encountered an error when loading image from bytes.\")\n\n    try:\n        image = Image.open(io.BytesIO(base64.b64decode(x.encode(\"ascii\"))))\n        return image\n    except Exception as e:\n        raise RuntimeError(\n            f\"Encountered an error when loading image from bytes starting \"\n            f\"with '{x[:20]}'. Expected either a PIL.Image.Image or a base64 \"\n            f\"encoded string of bytes.\"\n        ) from e\n</code></pre>"},{"location":"code_reference/mistral_common/image/#mistral_common.image.serialize_image_to_byte_str","title":"<code>serialize_image_to_byte_str(im, info)</code>","text":"<p>Serialize an image to a base64 encoded string of bytes.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>The image to serialize.</p> required <code>info</code> <code>SerializationInfo</code> <p>The serialization info.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The serialized image as a base64 encoded string of bytes.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def serialize_image_to_byte_str(im: Image.Image, info: SerializationInfo) -&gt; str:\n    r\"\"\"Serialize an image to a base64 encoded string of bytes.\n\n    Args:\n        im: The image to serialize.\n        info: The serialization info.\n\n    Returns:\n        The serialized image as a base64 encoded string of bytes.\n    \"\"\"\n    if hasattr(info, \"context\"):\n        context = info.context or {}\n    else:\n        context = {}\n\n    stream = io.BytesIO()\n    im_format = im.format or \"PNG\"\n    im.save(stream, format=im_format)\n    im_b64 = base64.b64encode(stream.getvalue()).decode(\"ascii\")\n    if context and (max_image_b64_len := context.get(\"max_image_b64_len\")):\n        return im_b64[:max_image_b64_len] + \"...\"\n    if context and context.get(\"add_format_prefix\"):\n        im_b64 = f\"data:image/{im_format.lower()};base64,\" + im_b64\n    return im_b64\n</code></pre>"},{"location":"code_reference/mistral_common/multimodal/","title":"multimodal","text":""},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal","title":"<code>mistral_common.multimodal</code>","text":""},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal.BeforeValidator","title":"<code>BeforeValidator(func, json_schema_input_type=PydanticUndefined)</code>  <code>dataclass</code>","text":"<p>!!! abstract \"Usage Documentation\"     field before validators</p> <p>A metadata class that indicates that a validation should be applied before the inner validation logic.</p> <p>Attributes:</p> Name Type Description <code>func</code> <code>NoInfoValidatorFunction | WithInfoValidatorFunction</code> <p>The validator function.</p> <code>json_schema_input_type</code> <code>Any</code> <p>The input type of the function. This is only used to generate the appropriate JSON Schema (in validation mode).</p> Example <pre><code>from typing import Annotated\n\nfrom pydantic import BaseModel, BeforeValidator\n\nMyInt = Annotated[int, BeforeValidator(lambda v: v + 1)]\n\nclass Model(BaseModel):\n    a: MyInt\n\nprint(Model(a=1).a)\n#&gt; 2\n\ntry:\n    Model(a='a')\nexcept TypeError as e:\n    print(e)\n    #&gt; can only concatenate str (not \"int\") to str\n</code></pre>"},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal.PlainSerializer","title":"<code>PlainSerializer(func, return_type=PydanticUndefined, when_used='always')</code>  <code>dataclass</code>","text":"<p>Plain serializers use a function to modify the output of serialization.</p> <p>This is particularly helpful when you want to customize the serialization for annotated types. Consider an input of <code>list</code>, which will be serialized into a space-delimited string.</p> <pre><code>from typing import Annotated\n\nfrom pydantic import BaseModel, PlainSerializer\n\nCustomStr = Annotated[\n    list, PlainSerializer(lambda x: ' '.join(x), return_type=str)\n]\n\nclass StudentModel(BaseModel):\n    courses: CustomStr\n\nstudent = StudentModel(courses=['Math', 'Chemistry', 'English'])\nprint(student.model_dump())\n#&gt; {'courses': 'Math Chemistry English'}\n</code></pre> <p>Attributes:</p> Name Type Description <code>func</code> <code>SerializerFunction</code> <p>The serializer function.</p> <code>return_type</code> <code>Any</code> <p>The return type for the function. If omitted it will be inferred from the type annotation.</p> <code>when_used</code> <code>WhenUsed</code> <p>Determines when this serializer should be used. Accepts a string with values <code>'always'</code>, <code>'unless-none'</code>, <code>'json'</code>, and <code>'json-unless-none'</code>. Defaults to 'always'.</p>"},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal.PlainSerializer.__get_pydantic_core_schema__","title":"<code>__get_pydantic_core_schema__(source_type, handler)</code>","text":"<p>Gets the Pydantic core schema.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>Any</code> <p>The source type.</p> required <code>handler</code> <code>GetCoreSchemaHandler</code> <p>The <code>GetCoreSchemaHandler</code> instance.</p> required <p>Returns:</p> Type Description <code>CoreSchema</code> <p>The Pydantic core schema.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/functional_serializers.py</code> <pre><code>def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -&gt; core_schema.CoreSchema:\n    \"\"\"Gets the Pydantic core schema.\n\n    Args:\n        source_type: The source type.\n        handler: The `GetCoreSchemaHandler` instance.\n\n    Returns:\n        The Pydantic core schema.\n    \"\"\"\n    schema = handler(source_type)\n    if self.return_type is not PydanticUndefined:\n        return_type = self.return_type\n    else:\n        try:\n            # Do not pass in globals as the function could be defined in a different module.\n            # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n            # in locals that may contain a parent/rebuild namespace:\n            return_type = _decorators.get_callable_return_type(\n                self.func,\n                localns=handler._get_types_namespace().locals,\n            )\n        except NameError as e:\n            raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n    return_schema = None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n    schema['serialization'] = core_schema.plain_serializer_function_ser_schema(\n        function=self.func,\n        info_arg=_decorators.inspect_annotated_serializer(self.func, 'plain'),\n        return_schema=return_schema,\n        when_used=self.when_used,\n    )\n    return schema\n</code></pre>"},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal.download_image","title":"<code>download_image(url)</code>","text":"<p>Download an image from a URL and return it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image to download.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The downloaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def download_image(url: str) -&gt; Image.Image:\n    r\"\"\"Download an image from a URL and return it as a PIL Image.\n\n    Args:\n        url: The URL of the image to download.\n\n    Returns:\n       The downloaded image as a PIL Image object.\n    \"\"\"\n    headers = {\"User-Agent\": f\"mistral-common/{__version__}\"}\n    try:\n        # Make a request to download the image\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n\n        # Convert the image content to a PIL Image\n        img = Image.open(io.BytesIO(response.content))\n        return img\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error downloading the image from {url}: {e}.\")\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to PIL image: {e}\")\n</code></pre>"},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal.maybe_load_image_from_str_or_bytes","title":"<code>maybe_load_image_from_str_or_bytes(x)</code>","text":"<p>Load an image from a string or bytes.</p> <p>If the input is already a PIL Image, return it as is.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Image, str, bytes]</code> <p>The input to load the image from. Can be a PIL Image, a string, or bytes. If it's a string, it's assumed to be a base64 encoded string of bytes.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The loaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def maybe_load_image_from_str_or_bytes(x: Union[Image.Image, str, bytes]) -&gt; Image.Image:\n    r\"\"\"Load an image from a string or bytes.\n\n    If the input is already a PIL Image, return it as is.\n\n    Args:\n        x: The input to load the image from. Can be a PIL Image, a string, or bytes.\n            If it's a string, it's assumed to be a base64 encoded string of bytes.\n\n    Returns:\n       The loaded image as a PIL Image object.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x\n    if isinstance(x, bytes):\n        try:\n            return Image.open(io.BytesIO(x))\n        except Exception:\n            raise RuntimeError(\"Encountered an error when loading image from bytes.\")\n\n    try:\n        image = Image.open(io.BytesIO(base64.b64decode(x.encode(\"ascii\"))))\n        return image\n    except Exception as e:\n        raise RuntimeError(\n            f\"Encountered an error when loading image from bytes starting \"\n            f\"with '{x[:20]}'. Expected either a PIL.Image.Image or a base64 \"\n            f\"encoded string of bytes.\"\n        ) from e\n</code></pre>"},{"location":"code_reference/mistral_common/multimodal/#mistral_common.multimodal.serialize_image_to_byte_str","title":"<code>serialize_image_to_byte_str(im, info)</code>","text":"<p>Serialize an image to a base64 encoded string of bytes.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>The image to serialize.</p> required <code>info</code> <code>SerializationInfo</code> <p>The serialization info.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The serialized image as a base64 encoded string of bytes.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def serialize_image_to_byte_str(im: Image.Image, info: SerializationInfo) -&gt; str:\n    r\"\"\"Serialize an image to a base64 encoded string of bytes.\n\n    Args:\n        im: The image to serialize.\n        info: The serialization info.\n\n    Returns:\n        The serialized image as a base64 encoded string of bytes.\n    \"\"\"\n    if hasattr(info, \"context\"):\n        context = info.context or {}\n    else:\n        context = {}\n\n    stream = io.BytesIO()\n    im_format = im.format or \"PNG\"\n    im.save(stream, format=im_format)\n    im_b64 = base64.b64encode(stream.getvalue()).decode(\"ascii\")\n    if context and (max_image_b64_len := context.get(\"max_image_b64_len\")):\n        return im_b64[:max_image_b64_len] + \"...\"\n    if context and context.get(\"add_format_prefix\"):\n        im_b64 = f\"data:image/{im_format.lower()};base64,\" + im_b64\n    return im_b64\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/base/","title":"base","text":""},{"location":"code_reference/mistral_common/protocol/base/#mistral_common.protocol.base","title":"<code>mistral_common.protocol.base</code>","text":""},{"location":"code_reference/mistral_common/protocol/base/#mistral_common.protocol.base.BaseCompletionRequest","title":"<code>BaseCompletionRequest(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base class for completion requests.</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>float</code> <p>Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>top_p</code> <code>float</code> <p>Nucleus sampling parameter, top-p probability mass, between 0 and 1.</p> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate.</p> <code>random_seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = BaseCompletionRequest(temperature=0.7, top_p=0.9, max_tokens=100, random_seed=42)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/base/#mistral_common.protocol.base.UsageInfo","title":"<code>UsageInfo(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Usage statistics for a completion request.</p> <p>Attributes:</p> Name Type Description <code>prompt_tokens</code> <code>int</code> <p>Number of tokens in the prompt.</p> <code>completion_tokens</code> <code>Optional[int]</code> <p>Number of tokens in the generated completion.</p> <code>total_tokens</code> <code>int</code> <p>Total number of tokens used in the request (prompt + completion).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; info = UsageInfo(prompt_tokens=10, completion_tokens=20, total_tokens=30)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/utils/","title":"utils","text":""},{"location":"code_reference/mistral_common/protocol/utils/#mistral_common.protocol.utils","title":"<code>mistral_common.protocol.utils</code>","text":""},{"location":"code_reference/mistral_common/protocol/utils/#mistral_common.protocol.utils.random_uuid","title":"<code>random_uuid()</code>","text":"<p>Generate a random UUID.</p> Source code in <code>src/mistral_common/protocol/utils.py</code> <pre><code>def random_uuid() -&gt; str:\n    \"\"\"Generate a random UUID.\"\"\"\n    return str(uuid.uuid4().hex)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/embedding/request/","title":"request","text":""},{"location":"code_reference/mistral_common/protocol/embedding/request/#mistral_common.protocol.embedding.request","title":"<code>mistral_common.protocol.embedding.request</code>","text":""},{"location":"code_reference/mistral_common/protocol/embedding/request/#mistral_common.protocol.embedding.request.EmbeddingRequest","title":"<code>EmbeddingRequest(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>\"Embedding request model used to generate embeddings for the given input.</p> <p>See EmbeddingResponse for the response model.</p> <p>Attributes:</p> Name Type Description <code>input</code> <code>Union[str, List[str]]</code> <p>Text to embed.</p> <code>model</code> <code>str</code> <p>ID of the model to use.</p> <code>encoding_format</code> <code>Optional[str]</code> <p>The format to return the embeddings in.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = EmbeddingRequest(input=\"Hello world!\", model=\"mistral-embed\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/embedding/response/","title":"response","text":""},{"location":"code_reference/mistral_common/protocol/embedding/response/#mistral_common.protocol.embedding.response","title":"<code>mistral_common.protocol.embedding.response</code>","text":""},{"location":"code_reference/mistral_common/protocol/embedding/response/#mistral_common.protocol.embedding.response.EmbeddingObject","title":"<code>EmbeddingObject(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Embedding object returned by the API.</p> <p>Attributes:</p> Name Type Description <code>object</code> <code>str</code> <p>The type of the object returned.</p> <code>embedding</code> <code>List[float]</code> <p>The embedding vector.</p> <code>index</code> <code>int</code> <p>The index of the embedding in the input text.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; embedding_object = EmbeddingObject(\n...    object=\"embedding\",\n...    embedding=[0.1, 0.2, 0.3],\n...    index=0\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/embedding/response/#mistral_common.protocol.embedding.response.EmbeddingResponse","title":"<code>EmbeddingResponse(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>\"Embedding response returned by the API.</p> <p>See the EmbeddingRequest for the request body.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The ID of the embedding.</p> <code>object</code> <code>str</code> <p>The type of the object returned.</p> <code>data</code> <code>List[EmbeddingObject]</code> <p>List of embeddings.</p> <code>model</code> <code>str</code> <p>The model used to generate the embeddings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response = EmbeddingResponse(\n...    id=\"embd-123\",\n...    object=\"list\",\n...    data=[],\n...    model=\"text-embedding-ada-002\",\n...    usage=UsageInfo(prompt_tokens=1, total_tokens=1, completion_tokens=0)\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/converters/","title":"converters","text":""},{"location":"code_reference/mistral_common/protocol/instruct/converters/#mistral_common.protocol.instruct.converters","title":"<code>mistral_common.protocol.instruct.converters</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/converters/#mistral_common.protocol.instruct.converters.convert_openai_messages","title":"<code>convert_openai_messages(messages)</code>","text":"<p>Convert OpenAI messages to Mistral messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]]</code> <p>The OpenAI messages to convert.</p> required <p>Returns:</p> Type Description <code>List[ChatMessage]</code> <p>The Mistral messages.</p> Source code in <code>src/mistral_common/protocol/instruct/converters.py</code> <pre><code>def convert_openai_messages(\n    messages: List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]],\n) -&gt; List[ChatMessage]:\n    r\"\"\"Convert OpenAI messages to Mistral messages.\n\n    Args:\n        messages: The OpenAI messages to convert.\n\n    Returns:\n        The Mistral messages.\n    \"\"\"\n    converted_messages: List[ChatMessage] = []\n    for openai_message in messages:\n        message_role = openai_message.get(\"role\")\n        message: ChatMessage\n        if message_role == \"user\":\n            message = UserMessage.from_openai(openai_message)\n        elif message_role == \"assistant\":\n            message = AssistantMessage.from_openai(openai_message)\n        elif message_role == \"tool\":\n            message = ToolMessage.from_openai(openai_message)\n        elif message_role == \"system\":\n            message = SystemMessage.from_openai(openai_message)\n        else:\n            raise ValueError(f\"Unknown message role: {message_role}\")\n        converted_messages.append(message)\n    return converted_messages\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/converters/#mistral_common.protocol.instruct.converters.convert_openai_tools","title":"<code>convert_openai_tools(tools)</code>","text":"<p>Convert OpenAI tools to Mistral tools.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>List[Dict[str, Any]]</code> <p>The OpenAI tools to convert.</p> required <p>Returns:</p> Type Description <code>List[Tool]</code> <p>The Mistral tools.</p> Source code in <code>src/mistral_common/protocol/instruct/converters.py</code> <pre><code>def convert_openai_tools(\n    tools: List[Dict[str, Any]],\n) -&gt; List[Tool]:\n    r\"\"\"Convert OpenAI tools to Mistral tools.\n\n    Args:\n        tools: The OpenAI tools to convert.\n\n    Returns:\n        The Mistral tools.\n    \"\"\"\n    converted_tools = [Tool.from_openai(openai_tool) for openai_tool in tools]\n    return converted_tools\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/","title":"messages","text":""},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages","title":"<code>mistral_common.protocol.instruct.messages</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.AssistantMessage","title":"<code>AssistantMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>Assistant message.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal[assistant]</code> <p>The role of the message.</p> <code>content</code> <code>Optional[str]</code> <p>The content of the message.</p> <code>tool_calls</code> <code>Optional[List[ToolCall]]</code> <p>The tool calls of the message.</p> <code>prefix</code> <code>bool</code> <p>Whether the message is a prefix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = AssistantMessage(content=\"Hello, how can I help you?\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.AssistantMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls, openai_message: Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]\n) -&gt; \"AssistantMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    openai_tool_calls = openai_message.get(\"tool_calls\", None)\n    tools_calls = (\n        [\n            ToolCall.from_openai(openai_tool_call)  # type: ignore[arg-type]\n            for openai_tool_call in openai_tool_calls\n        ]\n        if openai_tool_calls is not None\n        else None\n    )\n\n    return cls.model_validate(\n        {\n            \"role\": openai_message[\"role\"],\n            \"content\": openai_message.get(\"content\"),\n            \"tool_calls\": tools_calls,\n        }\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.AssistantMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    out_dict: dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]] = {\n        \"role\": self.role,\n    }\n    if self.content is not None:\n        out_dict[\"content\"] = self.content\n    if self.tool_calls is not None:\n        out_dict[\"tool_calls\"] = [tool_call.to_openai() for tool_call in self.tool_calls]\n\n    return out_dict\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseContentChunk","title":"<code>BaseContentChunk(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base class for all content chunks.</p> <p>Content chunks are used to send different types of content to the model.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[text, image, image_url]</code> <p>The type of the chunk.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseContentChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: Dict[str, Union[str, Dict[str, str]]]) -&gt; \"BaseContentChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"from_openai method not implemented for {cls.__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseContentChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, Dict[str, str]]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"to_openai method not implemented for {type(self).__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseMessage","title":"<code>BaseMessage(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base class for all messages.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal[system, user, assistant, tool]</code> <p>The role of the message.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls, openai_message: Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]\n) -&gt; \"BaseMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"from_openai method not implemented for {cls.__name__}.\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"to_openai method not implemented for {type(self).__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ChunkTypes","title":"<code>ChunkTypes</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the types of chunks that can be sent to the model.</p> <p>Attributes:</p> Name Type Description <code>text</code> <p>A text chunk.</p> <code>image</code> <p>An image chunk.</p> <code>image_url</code> <p>An image url chunk.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import ChunkTypes\n&gt;&gt;&gt; chunk_type = ChunkTypes.text\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.FinetuningAssistantMessage","title":"<code>FinetuningAssistantMessage(**data)</code>","text":"<p>               Bases: <code>AssistantMessage</code></p> <p>Assistant message for finetuning.</p> <p>Attributes:</p> Name Type Description <code>weight</code> <code>Optional[float]</code> <p>The weight of the message to train on.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = FinetuningAssistantMessage(content=\"Hello, how can I help you?\", weight=0.5)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageChunk","title":"<code>ImageChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Image chunk.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>SerializableImage</code> <p>The image to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; image_chunk = ImageChunk(image=Image.new('RGB', (200, 200), color='blue'))\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: Dict[str, Union[str, Dict[str, str]]]) -&gt; \"ImageChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    assert openai_chunk.get(\"type\") == \"image_url\", openai_chunk\n\n    image_url_dict = openai_chunk[\"image_url\"]\n    assert isinstance(image_url_dict, dict) and \"url\" in image_url_dict, image_url_dict\n\n    if re.match(r\"^data:image/\\w+;base64,\", image_url_dict[\"url\"]):  # Remove the prefix if it exists\n        image_url_dict[\"url\"] = image_url_dict[\"url\"].split(\",\")[1]\n\n    return cls.model_validate({\"image\": image_url_dict[\"url\"]})\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, Dict[str, str]]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    base64_image = self.model_dump(include={\"image\"}, context={\"add_format_prefix\": True})[\"image\"]\n    return {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image}}\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageURL","title":"<code>ImageURL(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Image URL or a base64 encoded image.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the image.</p> <code>detail</code> <code>Optional[str]</code> <p>The detail of the image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_url = ImageURL(url=\"https://example.com/image.png\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageURLChunk","title":"<code>ImageURLChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Image URL chunk.</p> <p>Attributes:</p> Name Type Description <code>image_url</code> <code>Union[ImageURL, str]</code> <p>The URL of the image or a base64 encoded image to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_url_chunk = ImageURLChunk(image_url=\"data:image/png;base64,iVBORw0\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageURLChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: Dict[str, Union[str, Dict[str, str]]]) -&gt; \"ImageURLChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate({\"image_url\": openai_chunk[\"image_url\"]})\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ImageURLChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, Dict[str, str]]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    image_url_dict = {\"url\": self.get_url()}\n    if isinstance(self.image_url, ImageURL) and self.image_url.detail is not None:\n        image_url_dict[\"detail\"] = self.image_url.detail\n\n    out_dict: Dict[str, Union[str, Dict[str, str]]] = {\n        \"type\": \"image_url\",\n        \"image_url\": image_url_dict,\n    }\n    return out_dict\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.Roles","title":"<code>Roles</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the roles of the messages.</p> <p>Attributes:</p> Name Type Description <code>system</code> <p>The system role.</p> <code>user</code> <p>The user role.</p> <code>assistant</code> <p>The assistant role.</p> <code>tool</code> <p>The tool role.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; role = Roles.user\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.SystemMessage","title":"<code>SystemMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>System message.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = SystemMessage(content=\"You are a helpful assistant.\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.SystemMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls, openai_message: Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]\n) -&gt; \"SystemMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    return cls.model_validate(openai_message)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.SystemMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.TextChunk","title":"<code>TextChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Text chunk.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The text to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text_chunk = TextChunk(text=\"Hello, how can I help you?\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.TextChunk.from_openai","title":"<code>from_openai(messages)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, messages: Dict[str, Union[str, Dict[str, str]]]) -&gt; \"TextChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate(messages)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.TextChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, Dict[str, str]]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ToolMessage","title":"<code>ToolMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>Tool message.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the message.</p> <code>tool_call_id</code> <code>Optional[str]</code> <p>The tool call id of the message.</p> <code>name</code> <code>Optional[str]</code> <p>The name of the tool. (Deprecated in V3 tokenization)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = ToolMessage(content=\"Hello, how can I help you?\", tool_call_id=\"123\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ToolMessage.from_openai","title":"<code>from_openai(messages)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, messages: Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]) -&gt; \"ToolMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    tool_message = cls.model_validate(messages)\n    assert tool_message.tool_call_id is not None, \"tool_call_id must be provided for tool messages.\"\n    return tool_message\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ToolMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    assert self.tool_call_id is not None, \"tool_call_id must be provided for tool messages.\"\n    return self.model_dump(exclude={\"name\"})\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.UserMessage","title":"<code>UserMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>User message.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Union[str, List[ContentChunk]]</code> <p>The content of the message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = UserMessage(content=\"Can you help me to write a poem?\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.UserMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls, openai_message: Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]\n) -&gt; \"UserMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    if isinstance(openai_message[\"content\"], str):\n        return cls.model_validate(openai_message)\n    return cls.model_validate(\n        {\n            \"role\": openai_message[\"role\"],\n            \"content\": [_convert_openai_content_chunks(chunk) for chunk in openai_message[\"content\"]],\n        },\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.UserMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    if isinstance(self.content, str):\n        return {\"role\": self.role, \"content\": self.content}\n    return {\"role\": self.role, \"content\": [chunk.to_openai() for chunk in self.content]}\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/","title":"normalize","text":""},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize","title":"<code>mistral_common.protocol.instruct.normalize</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizer","title":"<code>InstructRequestNormalizer(user_message_class, assistant_message_class, tool_message_class, system_message_class, instruct_request_class)</code>","text":"<p>               Bases: <code>Generic[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, InstructRequestType]</code></p> <p>Takes a ChatCompletionRequest and normalizes it into an InstructRequest.</p> <p>The normalization process does several things such as: - Aggregate consecutive messages of the same role - Aggregate system prompts - Normalize json content - Normalize tool calls</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>user_message_class</code> <code>Type[UserMessageType]</code> <p>The class for user messages.</p> required <code>assistant_message_class</code> <code>Type[AssistantMessageType]</code> <p>The class for assistant messages.</p> required <code>tool_message_class</code> <code>Type[ToolMessageType]</code> <p>The class for tool messages.</p> required <code>system_message_class</code> <code>Type[SystemMessageType]</code> <p>The class for system messages.</p> required <code>instruct_request_class</code> <code>Type[InstructRequestType]</code> <p>The class for instruct requests.</p> required Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def __init__(\n    self,\n    user_message_class: Type[UserMessageType],\n    assistant_message_class: Type[AssistantMessageType],\n    tool_message_class: Type[ToolMessageType],\n    system_message_class: Type[SystemMessageType],\n    instruct_request_class: Type[InstructRequestType],\n):\n    r\"\"\"Initializes the normalizer with the appropriate message classes.\n\n    Args:\n       user_message_class: The class for user messages.\n       assistant_message_class: The class for assistant messages.\n       tool_message_class: The class for tool messages.\n       system_message_class: The class for system messages.\n       instruct_request_class: The class for instruct requests.\n    \"\"\"\n    self._user_message_class = user_message_class\n    self._assistant_message_class = assistant_message_class\n    self._tool_message_class = tool_message_class\n    self._instruct_request_class = instruct_request_class\n    # this is unused but makes creation nicer\n    self._system_message_class = system_message_class\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizer.from_chat_completion_request","title":"<code>from_chat_completion_request(request)</code>","text":"<p>Converts a chat completion request to an instruct request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest[UATS]</code> <p>The chat completion request to convert.</p> required <p>Returns:</p> Type Description <code>InstructRequestType</code> <p>The converted instruct request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; request = ChatCompletionRequest(\n...     messages=[\n...         UserMessage(content=\"Hello\"),\n...         AssistantMessage(content=\"Hi\"),\n...     ],\n... )\n&gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n&gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def from_chat_completion_request(self, request: ChatCompletionRequest[UATS]) -&gt; InstructRequestType:\n    r\"\"\"Converts a chat completion request to an instruct request.\n\n    Args:\n        request: The chat completion request to convert.\n\n    Returns:\n        The converted instruct request.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n        &gt;&gt;&gt; request = ChatCompletionRequest(\n        ...     messages=[\n        ...         UserMessage(content=\"Hello\"),\n        ...         AssistantMessage(content=\"Hi\"),\n        ...     ],\n        ... )\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n        &gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n    \"\"\"\n    system_prompt = self._aggregate_system_prompts(request)\n    messages = self._aggregate_messages(request)\n\n    return self._instruct_request_class(\n        messages=messages,\n        system_prompt=system_prompt,\n        available_tools=request.tools,\n        continue_final_message=request.continue_final_message,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizer.normalizer","title":"<code>normalizer()</code>  <code>staticmethod</code>","text":"<p>Returns a normalizer for the default instruct request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>@staticmethod\ndef normalizer() -&gt; \"InstructRequestNormalizer\":\n    r\"\"\"Returns a normalizer for the default instruct request.\n\n    Examples:\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n    \"\"\"\n    return InstructRequestNormalizer(\n        UserMessage,\n        AssistantMessage,\n        ToolMessage,\n        SystemMessage,\n        InstructRequest[UATS, Tool],\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV13","title":"<code>InstructRequestNormalizerV13(user_message_class, assistant_message_class, tool_message_class, system_message_class, instruct_request_class)</code>","text":"<p>               Bases: <code>InstructRequestNormalizerV7</code></p> <p>Normalizer for the v13 tokenizer.</p> <p>It reorders tool messages based on the tool call order.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizerV13.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def __init__(\n    self,\n    user_message_class: Type[UserMessageType],\n    assistant_message_class: Type[AssistantMessageType],\n    tool_message_class: Type[ToolMessageType],\n    system_message_class: Type[SystemMessageType],\n    instruct_request_class: Type[InstructRequestType],\n):\n    r\"\"\"Initializes the normalizer with the appropriate message classes.\n\n    Args:\n       user_message_class: The class for user messages.\n       assistant_message_class: The class for assistant messages.\n       tool_message_class: The class for tool messages.\n       system_message_class: The class for system messages.\n       instruct_request_class: The class for instruct requests.\n    \"\"\"\n    self._user_message_class = user_message_class\n    self._assistant_message_class = assistant_message_class\n    self._tool_message_class = tool_message_class\n    self._instruct_request_class = instruct_request_class\n    # this is unused but makes creation nicer\n    self._system_message_class = system_message_class\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV13.normalizer","title":"<code>normalizer()</code>  <code>staticmethod</code>","text":"<p>Returns a normalizer for the default instruct request.</p> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>@staticmethod\ndef normalizer() -&gt; \"InstructRequestNormalizerV13\":\n    r\"\"\"Returns a normalizer for the default instruct request.\"\"\"\n    return InstructRequestNormalizerV13(\n        UserMessage,\n        AssistantMessage,\n        ToolMessage,\n        SystemMessage,\n        InstructRequest[UATS, Tool],\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV7","title":"<code>InstructRequestNormalizerV7(user_message_class, assistant_message_class, tool_message_class, system_message_class, instruct_request_class)</code>","text":"<p>               Bases: <code>InstructRequestNormalizer</code></p> <p>Normalizer for the v7 tokenizer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def __init__(\n    self,\n    user_message_class: Type[UserMessageType],\n    assistant_message_class: Type[AssistantMessageType],\n    tool_message_class: Type[ToolMessageType],\n    system_message_class: Type[SystemMessageType],\n    instruct_request_class: Type[InstructRequestType],\n):\n    r\"\"\"Initializes the normalizer with the appropriate message classes.\n\n    Args:\n       user_message_class: The class for user messages.\n       assistant_message_class: The class for assistant messages.\n       tool_message_class: The class for tool messages.\n       system_message_class: The class for system messages.\n       instruct_request_class: The class for instruct requests.\n    \"\"\"\n    self._user_message_class = user_message_class\n    self._assistant_message_class = assistant_message_class\n    self._tool_message_class = tool_message_class\n    self._instruct_request_class = instruct_request_class\n    # this is unused but makes creation nicer\n    self._system_message_class = system_message_class\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV7.from_chat_completion_request","title":"<code>from_chat_completion_request(request)</code>","text":"<p>Converts a chat completion request to an instruct request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest[UATS]</code> <p>The chat completion request to convert.</p> required <p>Returns:</p> Type Description <code>InstructRequestType</code> <p>The converted instruct request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; request = ChatCompletionRequest(\n...     messages=[\n...         UserMessage(content=\"Hello\"),\n...         AssistantMessage(content=\"Hi\"),\n...     ],\n... )\n&gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n&gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def from_chat_completion_request(self, request: ChatCompletionRequest[UATS]) -&gt; InstructRequestType:  # type: ignore[type-var]\n    r\"\"\"Converts a chat completion request to an instruct request.\n\n    Args:\n        request: The chat completion request to convert.\n\n    Returns:\n        The converted instruct request.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n        &gt;&gt;&gt; request = ChatCompletionRequest(\n        ...     messages=[\n        ...         UserMessage(content=\"Hello\"),\n        ...         AssistantMessage(content=\"Hi\"),\n        ...     ],\n        ... )\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n        &gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n    \"\"\"\n    messages = self._aggregate_messages(request)\n    return self._instruct_request_class(messages=messages, system_prompt=None, available_tools=request.tools)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV7.normalizer","title":"<code>normalizer()</code>  <code>staticmethod</code>","text":"<p>Returns a normalizer for the default instruct request</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>@staticmethod\ndef normalizer() -&gt; \"InstructRequestNormalizerV7\":\n    r\"\"\"Returns a normalizer for the default instruct request\n\n    Examples:\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n    \"\"\"\n    return InstructRequestNormalizerV7(\n        UserMessage,\n        AssistantMessage,\n        ToolMessage,\n        SystemMessage,\n        InstructRequest[UATS, Tool],\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.normalizer_for_tokenizer_version","title":"<code>normalizer_for_tokenizer_version(version)</code>","text":"<p>Gets the appropriate normalizer for the given tokenizer version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>TokenizerVersion</code> <p>The tokenizer version to get the normalizer for.</p> required <p>Returns:</p> Type Description <code>InstructRequestNormalizer</code> <p>The appropriate normalizer for the given tokenizer version.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = normalizer_for_tokenizer_version(TokenizerVersion.v1)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def normalizer_for_tokenizer_version(version: TokenizerVersion) -&gt; InstructRequestNormalizer:\n    r\"\"\"Gets the appropriate normalizer for the given tokenizer version.\n\n    Args:\n        version: The tokenizer version to get the normalizer for.\n\n    Returns:\n        The appropriate normalizer for the given tokenizer version.\n\n    Examples:\n        &gt;&gt;&gt; normalizer = normalizer_for_tokenizer_version(TokenizerVersion.v1)\n    \"\"\"\n    if version in {TokenizerVersion.v1, TokenizerVersion.v2, TokenizerVersion.v3}:\n        return InstructRequestNormalizer.normalizer()\n    elif version in {TokenizerVersion.v7, TokenizerVersion.v11}:\n        return InstructRequestNormalizerV7.normalizer()\n    elif version == TokenizerVersion.v13:\n        return InstructRequestNormalizerV13.normalizer()\n    raise ValueError(f\"Unknown tokenizer version {version}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/","title":"request","text":""},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request","title":"<code>mistral_common.protocol.instruct.request</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ChatCompletionRequest","title":"<code>ChatCompletionRequest(**data)</code>","text":"<p>               Bases: <code>BaseCompletionRequest</code>, <code>Generic[ChatMessageType]</code></p> <p>Request for a chat completion.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Optional[str]</code> <p>The model to use for the chat completion.</p> <code>messages</code> <code>List[ChatMessageType]</code> <p>The messages to use for the chat completion.</p> <code>response_format</code> <code>ResponseFormat</code> <p>The format of the response.</p> <code>tools</code> <code>Optional[List[Tool]]</code> <p>The tools to use for the chat completion.</p> <code>tool_choice</code> <code>ToolChoice</code> <p>The tool choice to use for the chat completion.</p> <code>truncate_for_context_length</code> <code>bool</code> <p>Whether to truncate the messages for the context length.</p> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import ToolTypes, Function\n&gt;&gt;&gt; request = ChatCompletionRequest(\n...     messages=[\n...         UserMessage(content=\"Hello!\"),\n...         AssistantMessage(content=\"Hi! How can I help you?\"),\n...     ],\n...     response_format=ResponseFormat(type=ResponseFormats.text),\n...     tools=[Tool(type=ToolTypes.function, function=Function(name=\"get_weather\", parameters={}))],\n...     tool_choice=ToolChoice.auto,\n...     truncate_for_context_length=True,\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ChatCompletionRequest.from_openai","title":"<code>from_openai(messages, tools=None, continue_final_message=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a chat completion request from the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]]</code> <p>The messages in the OpenAI format.</p> required <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The tools in the OpenAI format.</p> <code>None</code> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the constructor. These should be the same as the fields of the request class or the OpenAI API equivalent.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletionRequest</code> <p>The chat completion request.</p> Source code in <code>src/mistral_common/protocol/instruct/request.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls,\n    messages: List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]],\n    tools: Optional[List[Dict[str, Any]]] = None,\n    continue_final_message: bool = False,\n    **kwargs: Any,\n) -&gt; \"ChatCompletionRequest\":\n    r\"\"\"Create a chat completion request from the OpenAI format.\n\n    Args:\n        messages: The messages in the OpenAI format.\n        tools: The tools in the OpenAI format.\n        continue_final_message: Whether to continue the final message.\n        **kwargs: Additional keyword arguments to pass to the constructor. These should be the same as the fields\n            of the request class or the OpenAI API equivalent.\n\n\n    Returns:\n        The chat completion request.\n    \"\"\"\n    if \"seed\" in kwargs and \"random_seed\" in kwargs:\n        raise ValueError(\"Cannot specify both `seed` and `random_seed`.\")\n\n    random_seed = kwargs.pop(\"seed\", None) or kwargs.pop(\"random_seed\", None)\n\n    _check_openai_fields_names(set(cls.model_fields.keys()), set(kwargs.keys()))\n\n    converted_messages: list[ChatMessage] = convert_openai_messages(messages)\n\n    converted_tools = convert_openai_tools(tools) if tools is not None else None\n\n    return cls(\n        messages=converted_messages,  # type: ignore[arg-type]\n        tools=converted_tools,\n        random_seed=random_seed,\n        continue_final_message=continue_final_message,\n        **kwargs,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ChatCompletionRequest.to_openai","title":"<code>to_openai(**kwargs)</code>","text":"<p>Convert the request messages and tools into the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Additional parameters to be added to the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>The request in the OpenAI format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n&gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n&gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], temperature=0.15)\n&gt;&gt;&gt; request.to_openai(stream=True)\n{'temperature': 0.15, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'stream': True}\n&gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], tools=[\n...     Tool(function=Function(\n...         name=\"get_current_weather\",\n...         description=\"Get the current weather in a given location\",\n...         parameters={\n...             \"type\": \"object\",\n...             \"properties\": {\n...                 \"location\": {\n...                     \"type\": \"string\",\n...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n...                 },\n...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...             },\n...             \"required\": [\"location\"],\n...         },\n...     ),\n... )])\n&gt;&gt;&gt; request.to_openai()\n{'temperature': 0.7, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/request.py</code> <pre><code>def to_openai(self, **kwargs: Any) -&gt; Dict[str, List[Dict[str, Any]]]:\n    r\"\"\"Convert the request messages and tools into the OpenAI format.\n\n    Args:\n        kwargs: Additional parameters to be added to the request.\n\n    Returns:\n        The request in the OpenAI format.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n        &gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], temperature=0.15)\n        &gt;&gt;&gt; request.to_openai(stream=True)\n        {'temperature': 0.15, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'stream': True}\n        &gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], tools=[\n        ...     Tool(function=Function(\n        ...         name=\"get_current_weather\",\n        ...         description=\"Get the current weather in a given location\",\n        ...         parameters={\n        ...             \"type\": \"object\",\n        ...             \"properties\": {\n        ...                 \"location\": {\n        ...                     \"type\": \"string\",\n        ...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n        ...                 },\n        ...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n        ...             },\n        ...             \"required\": [\"location\"],\n        ...         },\n        ...     ),\n        ... )])\n        &gt;&gt;&gt; request.to_openai()\n        {'temperature': 0.7, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n    \"\"\"  # noqa: E501\n\n    # Handle messages and tools separately.\n    openai_request: Dict[str, Any] = self.model_dump(\n        exclude={\"messages\", \"tools\", \"truncate_for_context_length\"}, exclude_none=True\n    )\n\n    # Rename random_seed to seed.\n    seed = openai_request.pop(\"random_seed\", None)\n    if seed is not None:\n        openai_request[\"seed\"] = seed\n\n    if self.truncate_for_context_length:\n        raise NotImplementedError(\"Truncating for context length is not implemented for OpenAI requests.\")\n\n    for kwarg in kwargs:\n        # Check for duplicate keyword arguments.\n        if kwarg in openai_request:\n            raise ValueError(f\"Duplicate keyword argument: {kwarg}\")\n        # Check if kwarg should have been set in the request.\n        # This occurs when the field is different between the Mistral and OpenAI API.\n        elif kwarg in ChatCompletionRequest.model_fields:\n            raise ValueError(f\"Keyword argument {kwarg} is already set in the request.\")\n        # Check if kwarg is a valid OpenAI field name.\n        elif not _is_openai_field_name(kwarg):\n            raise ValueError(f\"Invalid keyword argument: {kwarg}, it should be an OpenAI field name.\")\n\n    openai_messages = []\n    for message in self.messages:\n        openai_messages.append(message.to_openai())\n\n    openai_request[\"messages\"] = openai_messages\n    if self.tools is not None:\n        openai_request[\"tools\"] = [tool.to_openai() for tool in self.tools]\n\n    openai_request.update(kwargs)\n\n    return openai_request\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ResponseFormat","title":"<code>ResponseFormat(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>The format of the response.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>ResponseFormats</code> <p>The type of the response.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response_format = ResponseFormat(type=ResponseFormats.text)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ResponseFormats","title":"<code>ResponseFormats</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of the different formats of an instruct response.</p> <p>Attributes:</p> Name Type Description <code>text</code> <p>The response is a plain text.</p> <code>json</code> <p>The response is a JSON object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response_format = ResponseFormats.text\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/","title":"response","text":""},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response","title":"<code>mistral_common.protocol.instruct.response</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.ChatCompletionResponse","title":"<code>ChatCompletionResponse(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A chat completion response.</p> <p>See ChatCompletionRequest for the request.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The id of the response.</p> <code>object</code> <code>str</code> <p>The object of the response.</p> <code>created</code> <code>int</code> <p>The creation time of the response.</p> <code>model</code> <code>str</code> <p>The model of the response.</p> <code>choices</code> <code>List[ChatCompletionResponseChoice]</code> <p>The choices of the response.</p> <code>usage</code> <code>UsageInfo</code> <p>The usage of the response.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response = ChatCompletionResponse(\n...     id=\"chatcmpl-123\",\n...     object=\"chat.completion\",\n...     created=1677652288,\n...     model=\"mistral-tiny\",\n...     choices=[\n...         ChatCompletionResponseChoice(index=0, message=DeltaMessage(role=\"user\", content=\"Hello, world!\"))\n...     ],\n...     usage=UsageInfo(prompt_tokens=10, total_tokens=20, completion_tokens=10),\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.ChatCompletionResponseChoice","title":"<code>ChatCompletionResponseChoice(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A choice in a chat completion.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>The index of the choice.</p> <code>message</code> <code>DeltaMessage</code> <p>The message of the choice.</p> <code>finish_reason</code> <code>Optional[FinishReason]</code> <p>The finish reason of the choice.</p> <code>logprobs</code> <code>Optional[ChatCompletionResponseChoiceLogprobs]</code> <p>The log probabilities of the choice.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; choice = ChatCompletionResponseChoice(index=0, message=DeltaMessage(role=\"user\", content=\"Hello, world!\"))\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.ChatCompletionResponseChoiceLogprobs","title":"<code>ChatCompletionResponseChoiceLogprobs(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Log probabilities for a choice.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>List[ChatCompletionTokenLogprobs]</code> <p>The log probabilities for the content.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; choice_logprobs = ChatCompletionResponseChoiceLogprobs(\n...     content=[ChatCompletionTokenLogprobs(token=\"hello\", logprob=-0.5, bytes=[104, 101, 108, 108, 111])]\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.ChatCompletionResponseStreamChoice","title":"<code>ChatCompletionResponseStreamChoice(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A choice in a chat completion stream response.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>The index of the choice.</p> <code>delta</code> <code>DeltaMessage</code> <p>The delta of the choice.</p> <code>finish_reason</code> <code>Optional[FinishReason]</code> <p>The finish reason of the choice.</p> <code>logprobs</code> <code>Optional[ChatCompletionResponseChoiceLogprobs]</code> <p>The log probabilities of the choice.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; choice = ChatCompletionResponseStreamChoice(\n...     index=0, delta=DeltaMessage(role=\"user\", content=\"Hello, world!\")\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.ChatCompletionStreamResponse","title":"<code>ChatCompletionStreamResponse(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A chat completion stream response.</p> <p>See ChatCompletionRequest for the request.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The id of the response.</p> <code>object</code> <code>str</code> <p>The object of the response.</p> <code>created</code> <code>int</code> <p>The creation time of the response.</p> <code>model</code> <code>str</code> <p>The model of the response.</p> <code>choices</code> <code>List[ChatCompletionResponseStreamChoice]</code> <p>The choices of the response.</p> <code>usage</code> <code>Optional[UsageInfo]</code> <p>The usage of the response.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response = ChatCompletionStreamResponse(\n...     id=\"chatcmpl-123\",\n...     object=\"chat.completion.chunk\",\n...     created=1677652288,\n...     model=\"mistral-tiny\",\n...     choices=[\n...         ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=\"user\", content=\"Hello, world!\"))\n...     ],\n...     usage=UsageInfo(prompt_tokens=10, total_tokens=20, completion_tokens=10),\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.ChatCompletionTokenLogprobs","title":"<code>ChatCompletionTokenLogprobs(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Log probabilities for a token.</p> <p>Attributes:</p> Name Type Description <code>token</code> <code>str</code> <p>The token.</p> <code>logprob</code> <code>float</code> <p>The log probability of the token.</p> <code>bytes</code> <code>List[int]</code> <p>The bytes of the token.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; token_logprobs = ChatCompletionTokenLogprobs(token=\"hello\", logprob=-0.5, bytes=[104, 101, 108, 108, 111])\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.DeltaMessage","title":"<code>DeltaMessage(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A message in a chat completion.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Optional[str]</code> <p>The role of the message.</p> <code>content</code> <code>Optional[str]</code> <p>The content of the message.</p> <code>tool_calls</code> <code>Optional[List[ToolCall]]</code> <p>The tool calls in the message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = DeltaMessage(role=\"user\", content=\"Hello, world!\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/response/#mistral_common.protocol.instruct.response.FinishReason","title":"<code>FinishReason</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Possible finish reasons.</p> <p>Attributes:</p> Name Type Description <code>stop</code> <p>The model hit a natural stop point or a provided stop sequence.</p> <code>length</code> <p>The maximum number of tokens specified in the request was reached.</p> <code>model_length</code> <p>The model hit its context length limit.</p> <code>error</code> <p>An error occurred during generation.</p> <code>tool_calls</code> <p>The model called a tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; reason = FinishReason.stop\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/","title":"tool_calls","text":""},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls","title":"<code>mistral_common.protocol.instruct.tool_calls</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.Function","title":"<code>Function(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Function definition for tools.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function.</p> <code>description</code> <code>str</code> <p>A description of what the function does.</p> <code>parameters</code> <code>Dict[str, Any]</code> <p>The parameters the functions accepts, described as a JSON Schema object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; function = Function(\n...     name=\"get_current_weather\",\n...     description=\"Get the current weather in a given location\",\n...     parameters={\n...         \"type\": \"object\",\n...         \"properties\": {\n...             \"location\": {\n...                 \"type\": \"string\",\n...                 \"description\": \"The city and state, e.g. San Francisco, CA\",\n...             },\n...             \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...         },\n...         \"required\": [\"location\"],\n...     },\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.FunctionCall","title":"<code>FunctionCall(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Function call.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function to call.</p> <code>arguments</code> <code>str</code> <p>The arguments to pass to the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; function_call = FunctionCall(\n...     name=\"get_current_weather\",\n...     arguments={\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"},\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.FunctionCall.validate_arguments","title":"<code>validate_arguments(v)</code>","text":"<p>Convert arguments to a JSON string if they are a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[str, Dict[str, Any]]</code> <p>The arguments to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The arguments as a JSON string.</p> Source code in <code>src/mistral_common/protocol/instruct/tool_calls.py</code> <pre><code>@field_validator(\"arguments\", mode=\"before\")\ndef validate_arguments(cls, v: Union[str, Dict[str, Any]]) -&gt; str:\n    \"\"\"Convert arguments to a JSON string if they are a dictionary.\n\n    Args:\n        v: The arguments to validate.\n\n    Returns:\n        The arguments as a JSON string.\n    \"\"\"\n    if isinstance(v, dict):\n        return json.dumps(v)\n    return v\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.Tool","title":"<code>Tool(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Tool definition.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>ToolTypes</code> <p>The type of the tool.</p> <code>function</code> <code>Function</code> <p>The function definition.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = Tool(\n...     function=Function(\n...         name=\"get_current_weather\",\n...         description=\"Get the current weather in a given location\",\n...         parameters={\n...             \"type\": \"object\",\n...             \"properties\": {\n...                 \"location\": {\n...                     \"type\": \"string\",\n...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n...                 },\n...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...             },\n...             \"required\": [\"location\"],\n...         },\n...     ),\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.ToolCall","title":"<code>ToolCall(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Tool call.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The ID of the tool call. Required for V3+ tokenization</p> <code>type</code> <code>ToolTypes</code> <p>The type of the tool call.</p> <code>function</code> <code>FunctionCall</code> <p>The function call.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool_call = ToolCall(\n...     id=\"call_abc123\",\n...     function=FunctionCall(\n...         name=\"get_current_weather\",\n...         arguments={\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"},\n...     ),\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.ToolChoice","title":"<code>ToolChoice</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of tool choice types.</p> <p>Attributes:</p> Name Type Description <code>auto</code> <p>Automatically choose the tool.</p> <code>none</code> <p>Do not use any tools.</p> <code>any</code> <p>Use any tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool_choice = ToolChoice.auto\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.ToolTypes","title":"<code>ToolTypes</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of tool types.</p> <p>Attributes:</p> Name Type Description <code>function</code> <p>A function tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool_type = ToolTypes.function\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/","title":"validator","text":""},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator","title":"<code>mistral_common.protocol.instruct.validator</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidator","title":"<code>MistralRequestValidator(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>Generic[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType]</code></p> <p>Validator for Mistral requests.</p> <p>This class validates the structure and content of Mistral requests.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; validator = MistralRequestValidator()\n&gt;&gt;&gt; messages = [UserMessage(content=\"Hello how are you ?\")]\n&gt;&gt;&gt; validator.validate_messages(messages, False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>ValidationMode</code> <p>The validation mode. Defaults to ValidationMode.test.</p> <code>test</code> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidator.validate_messages","title":"<code>validate_messages(messages, continue_final_message)</code>","text":"<p>Validates the list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[UATS]</code> <p>The list of messages to validate.</p> required <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; validator = MistralRequestValidator()\n&gt;&gt;&gt; messages = [AssistantMessage(content=\"Hi\"), UserMessage(content=\"Hello\")]\n&gt;&gt;&gt; validator.validate_messages(messages, False)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def validate_messages(self, messages: List[UATS], continue_final_message: bool) -&gt; None:\n    r\"\"\"Validates the list of messages.\n\n    Args:\n        messages: The list of messages to validate.\n        continue_final_message: Whether to continue the final message.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n        &gt;&gt;&gt; validator = MistralRequestValidator()\n        &gt;&gt;&gt; messages = [AssistantMessage(content=\"Hi\"), UserMessage(content=\"Hello\")]\n        &gt;&gt;&gt; validator.validate_messages(messages, False)\n    \"\"\"\n    self._validate_message_list_structure(messages, continue_final_message=continue_final_message)\n    self._validate_message_list_content(messages)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidator.validate_request","title":"<code>validate_request(request)</code>","text":"<p>Validates the request</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest</code> <p>The request to validate.</p> required <p>Returns:</p> Type Description <code>ChatCompletionRequest[UATS]</code> <p>The validated request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n&gt;&gt;&gt; validator = MistralRequestValidator()\n&gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello\")])\n&gt;&gt;&gt; validated_request = validator.validate_request(request)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def validate_request(self, request: ChatCompletionRequest) -&gt; ChatCompletionRequest[UATS]:\n    r\"\"\"Validates the request\n\n    Args:\n        request: The request to validate.\n\n    Returns:\n        The validated request.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n        &gt;&gt;&gt; validator = MistralRequestValidator()\n        &gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello\")])\n        &gt;&gt;&gt; validated_request = validator.validate_request(request)\n    \"\"\"\n\n    if self._mode == ValidationMode.serving:\n        if request.model is None:\n            raise InvalidRequestException(\"Model name parameter is required for serving mode\")\n\n    # Validate the messages\n    self.validate_messages(request.messages, continue_final_message=request.continue_final_message)\n\n    # Validate the tools\n    self._validate_tools(request.tools or [])\n\n    return request\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidatorV13","title":"<code>MistralRequestValidatorV13(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>MistralRequestValidatorV5</code></p> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidatorV3","title":"<code>MistralRequestValidatorV3(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>MistralRequestValidator</code></p> <p>Validator for v3 Mistral requests.</p> <p>This validator adds additional validation for tool call IDs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validator = MistralRequestValidatorV3()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidatorV5","title":"<code>MistralRequestValidatorV5(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>MistralRequestValidatorV3</code></p> <p>Validator for v5 Mistral requests.</p> <p>This validator allows for both tool calls and content in the assistant message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validator = MistralRequestValidatorV5()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.ValidationMode","title":"<code>ValidationMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for the validation mode.</p> <p>Attributes:</p> Name Type Description <code>serving</code> <p>The serving mode.</p> <code>finetuning</code> <p>The finetuning mode.</p> <code>test</code> <p>The test mode.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mode = ValidationMode.serving\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/instruct/request/","title":"request","text":""},{"location":"code_reference/mistral_common/tokens/instruct/request/#mistral_common.tokens.instruct.request","title":"<code>mistral_common.tokens.instruct.request</code>","text":""},{"location":"code_reference/mistral_common/tokens/instruct/request/#mistral_common.tokens.instruct.request.FIMRequest","title":"<code>FIMRequest(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A valid Fill in the Middle completion request to be tokenized.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str</code> <p>The prompt to be completed.</p> <code>suffix</code> <code>Optional[str]</code> <p>The suffix of the prompt. If provided, the model will generate text between the prompt and the suffix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = FIMRequest(prompt=\"Hello, my name is\", suffix=\" and I live in New York.\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/instruct/request/#mistral_common.tokens.instruct.request.InstructRequest","title":"<code>InstructRequest(**data)</code>","text":"<p>               Bases: <code>MistralBase</code>, <code>Generic[ChatMessageType, ToolType]</code></p> <p>A valid Instruct request to be tokenized.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[ChatMessageType]</code> <p>The history of the conversation.</p> <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt to be used for the conversation.</p> <code>available_tools</code> <code>Optional[List[ToolType]]</code> <p>The tools available to the assistant.</p> <code>truncate_at_max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to truncate the conversation at.</p> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, SystemMessage\n&gt;&gt;&gt; request = InstructRequest(\n...     messages=[UserMessage(content=\"Hello, how are you?\")], system_prompt=\"You are a helpful assistant.\"\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/instruct/request/#mistral_common.tokens.instruct.request.InstructRequest.from_openai","title":"<code>from_openai(messages, tools=None, continue_final_message=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an instruct request from the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]]</code> <p>The messages in the OpenAI format.</p> required <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The tools in the OpenAI format.</p> <code>None</code> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the constructor. These should be the same as the fields of the request class or the OpenAI API equivalent.</p> <code>{}</code> <p>Returns:</p> Type Description <code>InstructRequest</code> <p>The instruct request.</p> Source code in <code>src/mistral_common/tokens/instruct/request.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls,\n    messages: List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, Any]]]]]]],\n    tools: Optional[List[Dict[str, Any]]] = None,\n    continue_final_message: bool = False,\n    **kwargs: Any,\n) -&gt; \"InstructRequest\":\n    r\"\"\"Create an instruct request from the OpenAI format.\n\n    Args:\n        messages: The messages in the OpenAI format.\n        tools: The tools in the OpenAI format.\n        continue_final_message: Whether to continue the final message.\n        **kwargs: Additional keyword arguments to pass to the constructor. These should be the same as the fields\n            of the request class or the OpenAI API equivalent.\n\n    Returns:\n        The instruct request.\n    \"\"\"\n    # Handle the case where the tools are passed as `available_tools`.\n    # This is to maintain compatibility with the OpenAI API.\n    if \"available_tools\" in kwargs:\n        if tools is None:\n            tools = kwargs.pop(\"available_tools\")\n        else:\n            raise ValueError(\"Cannot specify both `tools` and `available_tools`.\")\n\n    _check_openai_fields_names(set(cls.model_fields.keys()), set(kwargs.keys()))\n\n    converted_messages: list[ChatMessage] = convert_openai_messages(messages)\n\n    converted_tools = convert_openai_tools(tools) if tools is not None else None\n\n    return cls(\n        messages=converted_messages,  # type: ignore[arg-type]\n        available_tools=converted_tools,  # type: ignore[arg-type]\n        continue_final_message=continue_final_message,\n        **kwargs,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/instruct/request/#mistral_common.tokens.instruct.request.InstructRequest.to_openai","title":"<code>to_openai(**kwargs)</code>","text":"<p>Convert the request messages and tools into the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Additional parameters to be added to the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>The request in the OpenAI format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n&gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n&gt;&gt;&gt; request = InstructRequest(messages=[UserMessage(content=\"Hello, how are you?\")])\n&gt;&gt;&gt; request.to_openai(temperature=0.15, stream=True)\n{'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'temperature': 0.15, 'stream': True}\n&gt;&gt;&gt; request = InstructRequest(\n...     messages=[UserMessage(content=\"Hello, how are you?\")],\n...     available_tools=[\n...     Tool(function=Function(\n...         name=\"get_current_weather\",\n...         description=\"Get the current weather in a given location\",\n...         parameters={\n...             \"type\": \"object\",\n...             \"properties\": {\n...                 \"location\": {\n...                     \"type\": \"string\",\n...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n...                 },\n...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...             },\n...             \"required\": [\"location\"],\n...         },\n...     ),\n... )])\n&gt;&gt;&gt; request.to_openai()\n{'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n</code></pre> Source code in <code>src/mistral_common/tokens/instruct/request.py</code> <pre><code>def to_openai(self, **kwargs: Any) -&gt; Dict[str, List[Dict[str, Any]]]:\n    r\"\"\"Convert the request messages and tools into the OpenAI format.\n\n    Args:\n        kwargs: Additional parameters to be added to the request.\n\n    Returns:\n        The request in the OpenAI format.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n        &gt;&gt;&gt; request = InstructRequest(messages=[UserMessage(content=\"Hello, how are you?\")])\n        &gt;&gt;&gt; request.to_openai(temperature=0.15, stream=True)\n        {'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'temperature': 0.15, 'stream': True}\n        &gt;&gt;&gt; request = InstructRequest(\n        ...     messages=[UserMessage(content=\"Hello, how are you?\")],\n        ...     available_tools=[\n        ...     Tool(function=Function(\n        ...         name=\"get_current_weather\",\n        ...         description=\"Get the current weather in a given location\",\n        ...         parameters={\n        ...             \"type\": \"object\",\n        ...             \"properties\": {\n        ...                 \"location\": {\n        ...                     \"type\": \"string\",\n        ...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n        ...                 },\n        ...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n        ...             },\n        ...             \"required\": [\"location\"],\n        ...         },\n        ...     ),\n        ... )])\n        &gt;&gt;&gt; request.to_openai()\n        {'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n    \"\"\"  # noqa: E501\n\n    # Handle messages, tools, and truncate_at_max_tokens separately.\n    openai_request: Dict[str, Any] = self.model_dump(\n        exclude={\"messages\", \"available_tools\", \"truncate_at_max_tokens\"}, exclude_none=True\n    )\n\n    for kwarg in kwargs:\n        # Check for duplicate keyword arguments.\n        if kwarg in openai_request:\n            raise ValueError(f\"Duplicate keyword argument: {kwarg}\")\n        # Check if kwarg should have been set in the request.\n        # This occurs when the field is different between the Mistral and OpenAI API.\n        elif kwarg in InstructRequest.model_fields:\n            raise ValueError(f\"Keyword argument {kwarg} is already set in the request.\")\n        # Check if the keyword argument is a valid OpenAI field name.\n        elif not _is_openai_field_name(kwarg):\n            raise ValueError(f\"Invalid keyword argument: {kwarg}, it should be an OpenAI field name.\")\n\n    openai_messages: list[dict[str, Any]] = []\n    if self.system_prompt is not None:\n        openai_messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n\n    for message in self.messages:\n        openai_messages.append(message.to_openai())\n\n    openai_request[\"messages\"] = openai_messages\n    if self.available_tools is not None:\n        # Rename available_tools to tools\n        openai_request[\"tools\"] = [tool.to_openai() for tool in self.available_tools]\n\n    if self.truncate_at_max_tokens is not None:  # Rename to max_tokens\n        raise NotImplementedError(\"Truncating at max tokens is not implemented for OpenAI requests.\")\n\n    openai_request.update(kwargs)\n\n    return openai_request\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/","title":"base","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base","title":"<code>mistral_common.tokens.tokenizers.base</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer","title":"<code>InstructTokenizer(tokenizer, image_encoder)</code>","text":"<p>               Bases: <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Base class for instruct tokenizers.</p> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> <code>image_encoder</code> <code>Optional[ImageEncoder]</code> <p>The image encoder to use if any.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>Optional[ImageEncoder]</code> <p>The image encoder to use if any.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer, image_encoder: Optional[ImageEncoder]) -&gt; None:\n    r\"\"\"Initialize the instruct tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use if any.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>  <code>abstractmethod</code>","text":"<p>Convert token ids to string</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The token ids to decode.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy to use for special tokens. Passing <code>None</code> will default to <code>self._special_token_policy</code> for Tekkenizer and <code>SpecialTokenPolicy.IGNORE</code> for SentencePieceTokenizer. Note that passing <code>None</code> will be deprecated and <code>special_token_policy</code> will default to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef decode(self, tokens: List[int], special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; str:\n    r\"\"\"Convert token ids to string\n\n    Args:\n        tokens: The token ids to decode.\n        special_token_policy: The policy to use for special tokens.\n            Passing `None` will default to `self._special_token_policy` for\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer] and `SpecialTokenPolicy.IGNORE`\n            for [SentencePieceTokenizer][mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer].\n            Note that passing `None` will be deprecated and `special_token_policy` will default to\n            `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_fim","title":"<code>encode_fim(request)</code>  <code>abstractmethod</code>","text":"<p>FIM request to Tokenized object</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FIMRequestType</code> <p>The FIM request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The tokenized FIM request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_fim(self, request: FIMRequestType) -&gt; TokenizedType:\n    r\"\"\"FIM request to Tokenized object\n\n    Args:\n        request: The FIM request to encode.\n\n    Returns:\n        The tokenized FIM request.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_instruct","title":"<code>encode_instruct(request)</code>  <code>abstractmethod</code>","text":"<p>Instruct request to Tokenized object</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstructRequestType</code> <p>The instruct request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The tokenized instruct request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_instruct(self, request: InstructRequestType) -&gt; TokenizedType:\n    r\"\"\"Instruct request to Tokenized object\n\n    Args:\n        request: The instruct request to encode.\n\n    Returns:\n        The tokenized instruct request.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>  <code>abstractmethod</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, List[ContentChunk]]</code> <p>The user content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the content is the last one.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_user_content(\n    self,\n    content: Union[str, List[ContentChunk]],\n    is_last: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The user content to encode.\n        is_last: Whether the content is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and images.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>  <code>abstractmethod</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The user message to encode.</p> required <code>available_tools</code> <code>Optional[List[Tool]]</code> <p>The available tools.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>is_first</code> <code>bool</code> <p>Whether the message is the first one.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: Optional[List[Tool]],\n    is_last: bool,\n    is_first: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The user message to encode.\n        available_tools: The available tools.\n        is_last: Whether the message is the last one.\n        is_first: Whether the message is the first one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and images.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.SpecialTokenPolicy","title":"<code>SpecialTokenPolicy</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>What to do with special tokens when encoding/decoding.</p> <p>Attributes:</p> Name Type Description <code>IGNORE</code> <p>Ignore special tokens.</p> <code>KEEP</code> <p>Keep special tokens.</p> <code>RAISE</code> <p>Raise an error if special tokens are found.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.SpecialTokens","title":"<code>SpecialTokens</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>[DEPRECATED] Enum of special tokens used in the tokenizer.</p> <p>Attributes:</p> Name Type Description <code>unk</code> <p>The unknown token.</p> <code>bos</code> <p>The beginning of string token.</p> <code>eos</code> <p>The end of string token.</p> <code>begin_inst</code> <p>The beginning of instruction token.</p> <code>end_inst</code> <p>The end of instruction token.</p> <code>begin_tools</code> <p>The beginning of tools token.</p> <code>end_tools</code> <p>The end of tools token.</p> <code>begin_tool_results</code> <p>The beginning of tool results token.</p> <code>end_tool_results</code> <p>The end of tool results token.</p> <code>tool_calls</code> <p>The tool calls token.</p> <code>img</code> <p>The image token.</p> <code>pad</code> <p>The pad token.</p> <code>img_break</code> <p>The image break token.</p> <code>img_end</code> <p>The image end token.</p> <code>prefix</code> <p>The prefix token for FIM.</p> <code>middle</code> <p>The middle token for FIM.</p> <code>suffix</code> <p>The suffix token for FIM.</p> <code>begin_system</code> <p>The beginning of system prompt token.</p> <code>end_system</code> <p>The end of system prompt token.</p> <code>begin_tool_content</code> <p>The beginning of tool content token.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unk = SpecialTokens.unk\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenized","title":"<code>Tokenized(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A tokenized <code>InstructRequest</code>.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>List[int]</code> <p>The token ids.</p> <code>text</code> <code>Optional[str]</code> <p>The text representation of the tokens.</p> <code>prefix_ids</code> <code>Optional[List[int]]</code> <p>The prefix ids for FIM.</p> <code>images</code> <code>List[ndarray]</code> <p>The loaded images associated with the tokens.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokenized = Tokenized(tokens=[1, 2, 3], text=\"Hello world\", prefix_ids=[1], images=[])\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.bos_id","title":"<code>bos_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the Beginning of String token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.eos_id","title":"<code>eos_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the End of String token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.file_path","title":"<code>file_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The file path of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.n_words","title":"<code>n_words</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Vocabulary size of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.pad_id","title":"<code>pad_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the Pad token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.unk_id","title":"<code>unk_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the Unk token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.version","title":"<code>version</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>  <code>abstractmethod</code>","text":"<p>Decode the token ids to a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The token ids to decode.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy to use for special tokens. Passing <code>None</code> will default to <code>self._special_token_policy</code> for Tekkenizer and <code>SpecialTokenPolicy.IGNORE</code> for SentencePieceTokenizer. Note that passing <code>None</code> will be deprecated and <code>special_token_policy</code> will default to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef decode(self, tokens: List[int], special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; str:\n    r\"\"\"Decode the token ids to a string.\n\n    Args:\n        tokens: The token ids to decode.\n        special_token_policy: The policy to use for special tokens.\n            Passing `None` will default to `self._special_token_policy` for\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer] and `SpecialTokenPolicy.IGNORE`\n            for [SentencePieceTokenizer][mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer].\n            Note that passing `None` will be deprecated and `special_token_policy` will default to\n            `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.encode","title":"<code>encode(s, bos, eos)</code>  <code>abstractmethod</code>","text":"<p>Convert a string to a list of token ids.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode(self, s: str, bos: bool, eos: bool) -&gt; List[int]:\n    \"\"\"Convert a string to a list of token ids.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.get_control_token","title":"<code>get_control_token(s)</code>  <code>abstractmethod</code>","text":"<p>Get the id of a control token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef get_control_token(self, s: str) -&gt; int:\n    r\"\"\"Get the id of a control token.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.id_to_piece","title":"<code>id_to_piece(token_id)</code>  <code>abstractmethod</code>","text":"<p>Convert a token id to the token str.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef id_to_piece(self, token_id: int) -&gt; str:\n    r\"\"\"Convert a token id to the token str.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.to_string","title":"<code>to_string(tokens)</code>  <code>abstractmethod</code>","text":"<p>[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.</p> <p>Use <code>decode</code> with <code>special_token_policy=SpecialTokenPolicy.KEEP</code> instead.</p> <p>This is a convenient method for debugging.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef to_string(self, tokens: List[int]) -&gt; str:\n    r\"\"\"[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.\n\n    Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\n\n    This is a convenient method for debugging.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.vocab","title":"<code>vocab()</code>  <code>abstractmethod</code>","text":"<p>All tokens in the vocabulary as strings.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef vocab(self) -&gt; List[str]:\n    r\"\"\"All tokens in the vocabulary as strings.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.TokenizerVersion","title":"<code>TokenizerVersion</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of tokenizer versions.</p> <p>Allow to distinguish between different versions of the tokenizer and maintain backward compatibility.</p> <p>Attributes:</p> Name Type Description <code>v1</code> <p>The first version of the tokenizer.</p> <code>v2</code> <p>The second version of the tokenizer that includes special control tokens [INST], [\\INST].</p> <code>v3</code> <p>The third version of the tokenizer that includes improved function calling.</p> <code>v7</code> <p>The seventh version of the tokenizer that includes improved system prompt and function calling.</p> <code>v11</code> <p>The eleventh version of the tokenizer that includes improved function calling.</p> <code>v13</code> <p>The thirteenth version of the tokenizer that includes no call id tokenization and better prompt caching.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; version = TokenizerVersion.v1\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.UserMessagePosition","title":"<code>UserMessagePosition</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Where to encode available tools</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/","title":"image","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image","title":"<code>mistral_common.tokens.tokenizers.image</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageConfig","title":"<code>ImageConfig(image_patch_size, max_image_size, spatial_merge_size=1)</code>  <code>dataclass</code>","text":"<p>Configuration for the image tokenizers.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageEncoder","title":"<code>ImageEncoder(image_config, special_ids)</code>","text":"<p>Image encoder for the image tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>image_config</code> <code>ImageConfig</code> <p>Configuration for the image tokenizer.</p> required <code>special_ids</code> <code>SpecialImageIDs</code> <p>Special image tokens ids.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def __init__(self, image_config: ImageConfig, special_ids: SpecialImageIDs) -&gt; None:\n    r\"\"\"Initialize the image encoder.\n\n    Args:\n        image_config: Configuration for the image tokenizer.\n        special_ids: Special image tokens ids.\n    \"\"\"\n    self.image_config = image_config\n    self.special_ids = special_ids\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageEncoder.__call__","title":"<code>__call__(content)</code>","text":"<p>Converts an image chunk to an image encoding.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[ImageChunk, ImageURLChunk]</code> <p>image chunk to be converted.</p> required <p>Returns:</p> Type Description <code>ImageEncoding</code> <p>Image encoding.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def __call__(self, content: Union[ImageChunk, ImageURLChunk]) -&gt; ImageEncoding:\n    r\"\"\"Converts an image chunk to an image encoding.\n\n    Args:\n        content: image chunk to be converted.\n\n    Returns:\n        Image encoding.\n    \"\"\"\n    image = image_from_chunk(content)\n    w, h = self._image_to_num_tokens(image)\n    assert w &gt; 0\n    assert h &gt; 0\n    image_tokens = ([self.special_ids.img] * w + [self.special_ids.img_break]) * h\n    image_tokens[-1] = self.special_ids.img_end\n    new_image_size = (\n        w * self.image_config.image_patch_size * self.image_config.spatial_merge_size,\n        h * self.image_config.image_patch_size * self.image_config.spatial_merge_size,\n    )\n    processed_image = transform_image(image, new_image_size)\n    return ImageEncoding(tokens=image_tokens, image=processed_image)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageEncoding","title":"<code>ImageEncoding(tokens, image)</code>  <code>dataclass</code>","text":"<p>A tokenized image.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>List[int]</code> <p>The token ids.</p> <code>image</code> <code>ndarray</code> <p>The image as a numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; image_encoding = ImageEncoding(tokens=[1, 2, 3], image=np.array([[0., 0.5, 1.]]))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.MultiModalVersion","title":"<code>MultiModalVersion</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Version of the image tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.SpecialImageIDs","title":"<code>SpecialImageIDs(img, img_break, img_end)</code>  <code>dataclass</code>","text":"<p>Special image tokens ids.</p> <p>Attributes:</p> Name Type Description <code>img</code> <code>int</code> <p>The image token id.</p> <code>img_break</code> <code>int</code> <p>The image break token id.</p> <code>img_end</code> <code>int</code> <p>The image end token id.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; special_image_ids = SpecialImageIDs(img=1, img_break=2, img_end=3)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.image_from_chunk","title":"<code>image_from_chunk(chunk)</code>","text":"<p>Get a serializable image from a chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>Union[ImageURLChunk, ImageChunk]</code> <p>The chunk to get the image from.</p> required <p>Returns:</p> Type Description <code>SerializableImage</code> <p>The image as a PIL Image object.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def image_from_chunk(chunk: Union[ImageURLChunk, ImageChunk]) -&gt; SerializableImage:\n    r\"\"\"Get a serializable image from a chunk.\n\n    Args:\n        chunk: The chunk to get the image from.\n\n    Returns:\n        The image as a PIL Image object.\n    \"\"\"\n    if isinstance(chunk, ImageChunk):\n        return chunk.image\n    if chunk.get_url().startswith(\"data:image\"):\n        data = chunk.get_url().split(\",\")[1]\n        image_data = base64.b64decode(data)\n        return Image.open(BytesIO(image_data))\n    if chunk.get_url().startswith(\"file\"):\n        return Image.open(open(chunk.get_url().replace(\"file://\", \"\"), \"rb\"))\n    if chunk.get_url().startswith(\"http\"):\n        return download_image(chunk.get_url())\n\n    raise RuntimeError(f\"Unsupported image url scheme {chunk.get_url()}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.is_cv2_installed","title":"<code>is_cv2_installed()</code>","text":"<p>Check if OpenCV is installed.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def is_cv2_installed() -&gt; bool:\n    r\"\"\"Check if OpenCV is installed.\"\"\"\n    return _cv2_installed\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.normalize","title":"<code>normalize(np_image, mean, std)</code>","text":"<p>Normalize a tensor image with mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>np_image</code> <code>ndarray</code> <p>Image to be normalized.</p> required <code>mean</code> <code>Tuple[float, float, float]</code> <p>Mean for each channel.</p> required <code>std</code> <code>Tuple[float, float, float]</code> <p>Standard deviation for each channel.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized image with shape (C, H, W).</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def normalize(\n    np_image: np.ndarray,\n    mean: Tuple[float, float, float],\n    std: Tuple[float, float, float],\n) -&gt; np.ndarray:\n    r\"\"\"Normalize a tensor image with mean and standard deviation.\n\n    Args:\n        np_image: Image to be normalized.\n        mean: Mean for each channel.\n        std: Standard deviation for each channel.\n\n    Returns:\n        Normalized image with shape (C, H, W).\n    \"\"\"\n    np_image = np_image / 255.0\n\n    assert len(np_image.shape) == 3, f\"{np_image.shape=}\"\n    assert np_image.shape[2] == len(mean) == len(std), f\"{np_image.shape=}, {mean=}, {std=}\"\n\n    np_image = (np_image - mean) / std\n\n    return np_image.transpose(2, 0, 1)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.transform_image","title":"<code>transform_image(image, new_size)</code>","text":"<p>Transform an image to a numpy array with the given size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image to be transformed.</p> required <code>new_size</code> <code>Tuple[int, int]</code> <p>New size of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Transformed image with shape (C, H, W).</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def transform_image(image: Image.Image, new_size: Tuple[int, int]) -&gt; np.ndarray:\n    r\"\"\"Transform an image to a numpy array with the given size.\n\n    Args:\n        image: Image to be transformed.\n        new_size: New size of the image.\n\n    Returns:\n        Transformed image with shape (C, H, W).\n    \"\"\"\n    if not is_cv2_installed():\n        raise ImportError(\"OpenCV is required for this function. Install it with 'pip install mistral-common[opencv]'\")\n\n    np_image = cv2.resize(np.array(_convert_to_rgb(image), dtype=np.float32), new_size, interpolation=cv2.INTER_CUBIC)\n    return normalize(np_image, DATASET_MEAN, DATASET_STD)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/","title":"instruct","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct","title":"<code>mistral_common.tokens.tokenizers.instruct</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase","title":"<code>InstructTokenizerBase(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizer</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Base instruct tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>Optional[ImageEncoder]</code> <p>The image encoder to use if any.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer, image_encoder: Optional[ImageEncoder] = None):\n    r\"\"\"Initialize the instruct tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use if any.\n    \"\"\"\n    self.tokenizer = tokenizer\n    self.image_encoder = image_encoder\n    super().__init__(tokenizer, image_encoder)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decode tokens to a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The tokens to decode.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy to use for special tokens. Passing <code>None</code> will default to <code>self._special_token_policy</code> for Tekkenizer and <code>SpecialTokenPolicy.IGNORE</code> for SentencePieceTokenizer. Note that passing <code>None</code> will be deprecated and <code>special_token_policy</code> will default to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def decode(self, tokens: List[int], special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; str:\n    r\"\"\"Decode tokens to a string.\n\n    Args:\n        tokens: The tokens to decode.\n        special_token_policy: The policy to use for special tokens.\n            Passing `None` will default to `self._special_token_policy` for\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer] and `SpecialTokenPolicy.IGNORE`\n            for [SentencePieceTokenizer][mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer].\n            Note that passing `None` will be deprecated and `special_token_policy` will default to\n            `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    return self.tokenizer.decode(tokens, special_token_policy=special_token_policy)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>  <code>abstractmethod</code>","text":"<p>Encode an assistant message.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The assistant message is not implemented for the base tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@abstractmethod\ndef encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; List[int]:\n    r\"\"\"Encode an assistant message.\n\n    Raises:\n        NotImplementedError: The assistant message is not implemented for the base tokenizer.\n    \"\"\"\n    raise NotImplementedError(\"Assistant message not implemented\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_instruct","title":"<code>encode_instruct(request)</code>","text":"<p>Encode an instruct request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstructRequest[AssistantMessageType, Tool]</code> <p>The request to encode.</p> required <p>Returns:</p> Type Description <code>Tokenized</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_instruct(\n    self,\n    request: InstructRequest[AssistantMessageType, Tool],\n) -&gt; Tokenized:\n    r\"\"\"Encode an instruct request.\n\n    Args:\n        request: The request to encode.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    # init at bos\n    images: List[np.ndarray] = []\n    prefix_ids: Optional[List[int]] = None\n    tokens_list: List[Optional[List[int]]] = []\n\n    # find last user message\n    first_user_idx, last_user_idx = self.find_first_last_user(request)\n    for msg_idx, msg in enumerate(request.messages):\n        if (\n            request.continue_final_message\n            and (msg_idx == len(request.messages) - 1)\n            and not isinstance(msg, AssistantMessage)\n        ):\n            raise InvalidMessageStructureException(\n                \"Cannot continue final message if it is not an assistant message\"\n            )\n        if isinstance(msg, UserMessage):\n            new_tokens, new_images = self.encode_user_message(\n                msg,\n                request.available_tools,\n                msg_idx == last_user_idx,\n                msg_idx == first_user_idx,\n                system_prompt=request.system_prompt,\n                force_img_first=True,  # img is always first when providing text/img chunk pair\n            )\n            images.extend(new_images)\n        elif isinstance(msg, ToolMessage):\n            new_tokens = self.encode_tool_message(msg, msg_idx &lt; last_user_idx)\n        elif isinstance(msg, AssistantMessage):\n            continue_message = request.continue_final_message and (msg_idx == len(request.messages) - 1)\n\n            new_tokens = self.encode_assistant_message(\n                msg, msg_idx &lt; last_user_idx, continue_message=continue_message\n            )\n            if msg_idx == len(request.messages) - 1:\n                prefix_ids = new_tokens\n        elif isinstance(msg, SystemMessage):\n            new_tokens = self.encode_system_message(msg)\n        else:\n            raise TokenizerException(f\"Unknown message type {type(msg)}\")\n\n        tokens_list.append(new_tokens)\n\n    if request.truncate_at_max_tokens is not None:\n        self._truncate_for_max_tokens(\n            tokens_list,\n            request.messages,\n            request.truncate_at_max_tokens,\n            last_user_idx,\n        )\n    tokens = self.start()\n\n    for tok in tokens_list:\n        if tok is not None:\n            tokens.extend(tok)\n\n    return Tokenized(\n        tokens=tokens,\n        text=self.decode(tokens, special_token_policy=SpecialTokenPolicy.KEEP),\n        prefix_ids=prefix_ids,\n        images=images,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>  <code>abstractmethod</code>","text":"<p>Encode a tool message.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The tool message is not implemented for the base tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@abstractmethod\ndef encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; List[int]:\n    r\"\"\"Encode a tool message.\n\n    Raises:\n        NotImplementedError: The tool message is not implemented for the base tokenizer.\n    \"\"\"\n    raise NotImplementedError(\"Tool message not implemented\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.find_first_last_user","title":"<code>find_first_last_user(request)</code>  <code>staticmethod</code>","text":"<p>Find the first and last user message in the request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstructRequest</code> <p>The request to search for user messages.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>The index of the first and last user message.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@staticmethod\ndef find_first_last_user(request: InstructRequest) -&gt; Tuple[int, int]:\n    r\"\"\"Find the first and last user message in the request.\n\n    Args:\n        request: The request to search for user messages.\n\n    Returns:\n        The index of the first and last user message.\n    \"\"\"\n    last_user_idx = -1\n    first_user_idx = -1\n    for i, msg in list(enumerate(request.messages)):\n        if isinstance(msg, UserMessage):\n            if first_user_idx == -1:\n                first_user_idx = i\n            last_user_idx = i\n    return first_user_idx, last_user_idx\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.start","title":"<code>start()</code>","text":"<p>Return the start tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def start(self) -&gt; List[int]:\n    r\"\"\"Return the start tokens.\"\"\"\n    return [self.tokenizer.bos_id]\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1","title":"<code>InstructTokenizerV1(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerBase</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Instruct tokenizer V1.</p> <p>This tokenizer has basic for messages. It does not support tools or image inputs.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer, image_encoder: Optional[ImageEncoder] = None):\n    r\"\"\"Initialize the instruct tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use if any.\n    \"\"\"\n    self.tokenizer = tokenizer\n    self.image_encoder = image_encoder\n    super().__init__(tokenizer, image_encoder)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <code>continue_message</code> <code>bool</code> <p>Whether to continue the message generation. Only use this if the assistant message is the last message.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; List[int]:\n    r\"\"\"Encode an assistant message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert isinstance(message, AssistantMessage), message\n    if message.tool_calls is not None and len(message.tool_calls) &gt; 0:\n        raise TokenizerException(\"Tools not implemented for tokenizer V1\")\n    if continue_message and message.prefix:\n        raise InvalidAssistantMessageException(\n            \"`continue_message` is only supported for assistant messages that have `prefix=False`.\"\n        )\n    elif message.content:\n        curr_tokens = self.tokenizer.encode(message.content, bos=False, eos=False)\n    else:\n        raise TokenizerException(f\"{message.content} // {message.tool_calls}\")\n    if not message.prefix and not continue_message:\n        curr_tokens.append(self.tokenizer.eos_id)\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_fim","title":"<code>encode_fim(request)</code>","text":"<p>Encode a FIM request.</p> <p>Raises:</p> Type Description <code>TokenizerException</code> <p>The FIM request is not implemented for this version.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_fim(self, request: FIMRequest) -&gt; Tokenized:\n    r\"\"\"Encode a FIM request.\n\n    Raises:\n       TokenizerException: The FIM request is not implemented for this version.\n    \"\"\"\n    raise TokenizerException(\"FIM not available for tokenizer V1\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> <p>Raises:</p> Type Description <code>TokenizerException</code> <p>The tool message is not implemented for this version.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; List[int]:\n    r\"\"\"Encode a tool message.\n\n    Raises:\n        TokenizerException: The tool message is not implemented for this version.\n    \"\"\"\n    raise TokenizerException(\"Tools not implemented for tokenizer V1\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, List[ContentChunk]]</code> <p>The content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Not used.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and empty list.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_content(\n    self,\n    content: Union[str, List[ContentChunk]],\n    is_last: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The content to encode.\n        is_last: Whether the message is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Not used.\n\n    Returns:\n        The encoded tokens and empty list.\n    \"\"\"\n    assert isinstance(content, str)\n\n    if is_last and system_prompt:\n        content = system_prompt + \"\\n\\n\" + content\n\n    tokens = self.tokenizer.encode(content, bos=False, eos=False)\n    return tokens, []\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The message to encode.</p> required <code>available_tools</code> <code>Optional[List[Tool]]</code> <p>Not used.</p> required <code>is_last</code> <code>bool</code> <p>Not used.</p> required <code>is_first</code> <code>bool</code> <p>Whether the message is the first one.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Not used.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and empty list.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: Optional[List[Tool]],\n    is_last: bool,\n    is_first: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The message to encode.\n        available_tools: Not used.\n        is_last: Not used.\n        is_first: Whether the message is the first one.\n        system_prompt: The system prompt.\n        force_img_first: Not used.\n\n    Returns:\n        The encoded tokens and empty list.\n    \"\"\"\n    assert isinstance(message.content, str), \"Message content must be normalized\"\n    assert self.image_encoder is None, \"InstructTokenizerV1 cannot encode images\"\n\n    content = \"\"\n    if is_first and system_prompt:\n        content = system_prompt + \"\\n\\n\" + message.content\n    else:\n        content = message.content\n\n    message_txt = f\"[INST] {content} [/INST]\"\n    curr_tokens, image_tokens = self.encode_user_content(content=message_txt, is_last=False, system_prompt=None)\n    return curr_tokens, image_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV11","title":"<code>InstructTokenizerV11(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV7</code></p> <p>Instruct tokenizer V11.</p> <p>The difference with V7 tokenizer is that it encodes tool calls differently: Tool call results are encoded as : - [begin tool call] call_name_tokens [call id] call_id_tokens [args] content tokens</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: Optional[ImageEncoder] = None,\n) -&gt; None:\n    super().__init__(tokenizer, image_encoder)\n    self.ARGS = self.tokenizer.get_control_token(SpecialTokens.args.value)\n    self.CALL_ID = self.tokenizer.get_control_token(SpecialTokens.call_id.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV13","title":"<code>InstructTokenizerV13(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV11</code></p> <p>Instruct tokenizer V13.</p> The difference with V11 tokenizer is that it encodes tool calls differently <ul> <li>available tools are tokenized at the first user message.</li> <li>call id is no longer tokenized for tool calls or results.</li> </ul> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: Optional[ImageEncoder] = None,\n) -&gt; None:\n    super().__init__(tokenizer, image_encoder)\n    self.ARGS = self.tokenizer.get_control_token(SpecialTokens.args.value)\n    self.CALL_ID = self.tokenizer.get_control_token(SpecialTokens.call_id.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV13.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <p>Returns:     The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; List[int]:\n    r\"\"\"Encode a tool message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert message.tool_call_id is not None, \"Tool call id must be provided for tokenizer &gt;= v13\"\n\n    tokens = self.tokenizer.encode(message.content, bos=False, eos=False)\n    curr_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *tokens,\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2","title":"<code>InstructTokenizerV2(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV1</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Instruct tokenizer V2.</p> <p>This tokenizer adds supports to images, tools and FIM requests.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>Optional[ImageEncoder]</code> <p>The image encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer, image_encoder: Optional[ImageEncoder] = None):\n    r\"\"\"Initialize the tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use.\n    \"\"\"\n    super().__init__(tokenizer, image_encoder)\n    self.BEGIN_INST = self.tokenizer.get_control_token(SpecialTokens.begin_inst.value)\n    self.END_INST = self.tokenizer.get_control_token(SpecialTokens.end_inst.value)\n    self.BEGIN_AVAILABLE_TOOLS = self.tokenizer.get_control_token(SpecialTokens.begin_tools.value)\n    self.END_AVAILABLE_TOOLS = self.tokenizer.get_control_token(SpecialTokens.end_tools.value)\n    self.BEGIN_TOOL_RESULTS = self.tokenizer.get_control_token(SpecialTokens.begin_tool_results.value)\n    self.END_TOOL_RESULTS = self.tokenizer.get_control_token(SpecialTokens.end_tool_results.value)\n    self.TOOL_CALLS = self.tokenizer.get_control_token(SpecialTokens.tool_calls.value)\n    self.BOS = self.tokenizer.get_control_token(SpecialTokens.bos.value)\n    self.PREFIX = self.tokenizer.get_control_token(SpecialTokens.prefix.value)\n    self.SUFFIX = self.tokenizer.get_control_token(SpecialTokens.suffix.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Whether the message is before the last user message. If has tools and true, the message is not encoded.</p> required <code>continue_message</code> <code>bool</code> <p>Whether to continue the message generation. Only use this if the assistant message is the last message.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; List[int]:\n    r\"\"\"Encode an assistant message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Whether the message is before the last user message. If has tools and true, the\n            message is not encoded.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    if message.tool_calls and message.content:\n        raise ValueError(f\"Cannot have tool calls and content defined in the same assistant message {message}\")\n    if continue_message and message.prefix:\n        raise InvalidAssistantMessageException(\n            \"`continue_message` is only supported for assistant messages that have `prefix=False`.\"\n        )\n\n    if message.tool_calls:\n        if is_before_last_user_message:\n            # don't tokenize tool call before last user message\n            return []\n        curr_tokens = self._encode_tool_calls_in_assistant_message(message)\n    elif message.content:\n        curr_tokens = self._encode_normal_content_assistant_message(message)\n    else:\n        raise TokenizerException(f\"Invalid assistant message: {message.content}\")\n    if not message.prefix and not continue_message:\n        curr_tokens.append(self.tokenizer.eos_id)\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_fim","title":"<code>encode_fim(request)</code>","text":"<p>Encode a FIM request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FIMRequest</code> <p>The request to encode.</p> required <p>Returns:</p> Type Description <code>Tokenized</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_fim(self, request: FIMRequest) -&gt; Tokenized:\n    r\"\"\"Encode a FIM request.\n\n    Args:\n        request: The request to encode.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    prefix_tokens = self.tokenizer.encode(request.prompt, bos=False, eos=False)\n    suffix_tokens = self._encode_infilling(request.suffix) if request.suffix else []\n    tokens = [\n        self.BOS,\n        self.SUFFIX,\n        *suffix_tokens,\n        self.PREFIX,\n        *prefix_tokens,\n    ]\n    return Tokenized(tokens=tokens, text=self.decode(tokens, special_token_policy=SpecialTokenPolicy.KEEP))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Whether the message is before the last user message. If true, the message is not encoded.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; List[int]:\n    r\"\"\"Encode a tool message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Whether the message is before the last user message. If true, the message is\n            not encoded.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    if is_before_last_user_message:\n        # don't tokenize last tool response before last user msg\n        return []\n\n    # Currently only supports single tool results\n    tool_result_str = json.dumps([self._prepare_tool_result(message)], ensure_ascii=False)\n    curr_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *self.tokenizer.encode(tool_result_str, bos=False, eos=False),\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The message to encode.</p> required <code>available_tools</code> <code>Optional[List[Tool]]</code> <p>The list of available tools if any.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>is_first</code> <code>bool</code> <p>Not used.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and the list of images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: Optional[List[Tool]],\n    is_last: bool,\n    is_first: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The message to encode.\n        available_tools: The list of available tools if any.\n        is_last: Whether the message is the last one.\n        is_first: Not used.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the list of images.\n    \"\"\"\n    do_encode_tools = False\n    do_encode_tools |= is_first and (self._user_message_position_to_encode_tools == UserMessagePosition.first)\n    do_encode_tools |= is_last and (self._user_message_position_to_encode_tools == UserMessagePosition.last)\n    tools_tokens: List[int] = []\n\n    if do_encode_tools and available_tools:\n        tools = [tool.model_dump() for tool in available_tools]\n        tools_json_tokens = self.tokenizer.encode(json.dumps(tools, ensure_ascii=False), bos=False, eos=False)\n        tools_tokens = [\n            self.BEGIN_AVAILABLE_TOOLS,\n            *tools_json_tokens,\n            self.END_AVAILABLE_TOOLS,\n        ]\n\n    tokens, image_tokens = self.encode_user_content(\n        content=message.content,\n        is_last=is_last,\n        system_prompt=system_prompt,\n        force_img_first=force_img_first,\n    )\n\n    prefix_tokens = [*tools_tokens, self.BEGIN_INST]\n    suffix_tokens = [self.END_INST]\n\n    curr_tokens = prefix_tokens + tokens + suffix_tokens\n\n    return curr_tokens, image_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3","title":"<code>InstructTokenizerV3(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV2</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Instruct tokenizer V3.</p> <p>The only difference with V2 tokenizer is that it encodes the tool messages differently.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>Optional[ImageEncoder]</code> <p>The image encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer, image_encoder: Optional[ImageEncoder] = None) -&gt; None:\n    r\"\"\"Initialize the tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use.\n    \"\"\"\n    super().__init__(tokenizer, image_encoder=image_encoder)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> Note <p>Same as V2 but always encode the tool history. continue_message: Whether to continue the message generation.     Only use this if the assistant message is the last message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; List[int]:\n    r\"\"\"Encode an assistant message.\n\n    Note:\n        Same as [V2][mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_assistant_message] but\n        always encode the tool history.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    return super().encode_assistant_message(message, False, continue_message)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> Note <p>Same as V2 but tools are not wrapped in a list and the history is also tokenized.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Whether the message is before the last user message. If true, the message is not encoded.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; List[int]:\n    r\"\"\"Encode a tool message.\n\n    Note:\n        Same as [V2][mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_tool_message] but tools\n        are not wrapped in a list and the history is also tokenized.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Whether the message is before the last user message. If true, the message is\n            not encoded.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    tool_result_str = json.dumps(self._prepare_tool_result(message), ensure_ascii=False)\n    curr_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *self.tokenizer.encode(tool_result_str, bos=False, eos=False),\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, List[ContentChunk]]</code> <p>The content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and the images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_content(\n    self,\n    content: Union[str, List[ContentChunk]],\n    is_last: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The content to encode.\n        is_last: Whether the message is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the images.\n    \"\"\"\n    if isinstance(content, str):\n        return super().encode_user_content(content, is_last, system_prompt)\n\n    tokens: List[int] = []\n    images: List[np.ndarray] = []\n\n    has_one_img_one_text_first = (\n        len(content) == 2\n        and isinstance(content[0], TextChunk)\n        and isinstance(content[1], (ImageURLChunk, ImageChunk))\n    )\n    if force_img_first and has_one_img_one_text_first:\n        # make sure that if exactly one image and text chunk are passed we force the image chunk to be first\n        content = [content[1], content[0]]\n\n    first_chunk = True\n    for chunk in content:\n        content = \"\"\n        if first_chunk and is_last and system_prompt:\n            first_chunk = False\n            content = system_prompt + \"\\n\\n\"\n        if isinstance(chunk, TextChunk):\n            content += chunk.text\n            tokens.extend(self.tokenizer.encode(content, bos=False, eos=False))\n        else:\n            assert self.image_encoder is not None, \"Make sure to define a image encoder at init\"\n            if content:\n                tokens.extend(self.tokenizer.encode(content, bos=False, eos=False))\n\n            img_encoding = self.image_encoder(chunk)\n\n            tokens.extend(img_encoding.tokens)\n            images.append(img_encoding.image)\n\n    return tokens, images\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7","title":"<code>InstructTokenizerV7(tokenizer, image_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV3</code></p> <p>Instruct tokenizer V7.</p> <p>The difference with V3 tokenizer is that it encodes the system prompts differently: - in V7 the system prompts are treated as separate SystemMessages - they are no longer prepended to the last user message - they are printed between special tokens</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>Optional[ImageEncoder]</code> <p>The image encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer, image_encoder: Optional[ImageEncoder] = None) -&gt; None:\n    r\"\"\"Initialize the tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use.\n    \"\"\"\n\n    super().__init__(tokenizer, image_encoder)\n    self.BEGIN_SYSTEM = self.tokenizer.get_control_token(SpecialTokens.begin_system.value)\n    self.END_SYSTEM = self.tokenizer.get_control_token(SpecialTokens.end_system.value)\n    self.BEGIN_TOOL_CONTENT = self.tokenizer.get_control_token(SpecialTokens.begin_tool_content.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <code>continue_message</code> <code>bool</code> <p>Whether to continue the message generation. Only use this if the assistant message is the last message.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; List[int]:\n    r\"\"\"Encode an assistant message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    if not message.content and not message.tool_calls:\n        raise TokenizerException(f\"Invalid assistant message: {message}\")\n    if continue_message and message.prefix:\n        raise InvalidAssistantMessageException(\n            \"`continue_message` is only supported for assistant messages that have `prefix=False`.\"\n        )\n\n    curr_tokens: list = []\n    if message.content:\n        curr_tokens += self._encode_normal_content_assistant_message(message)\n    if message.tool_calls:\n        curr_tokens += self._encode_tool_calls_in_assistant_message(message)\n    if not message.prefix and not continue_message:\n        curr_tokens.append(self.tokenizer.eos_id)\n\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_system_message","title":"<code>encode_system_message(message)</code>","text":"<p>Encode a system message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>SystemMessage</code> <p>The message to encode.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_system_message(self, message: SystemMessage) -&gt; List[int]:\n    r\"\"\"Encode a system message.\n\n    Args:\n        message: The message to encode.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    tokens = [\n        self.BEGIN_SYSTEM,\n        *self.tokenizer.encode(message.content, bos=False, eos=False),\n        self.END_SYSTEM,\n    ]\n    return tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> Note <p>Same as V3 but tools are not wrapped in a list and history is also tokenized</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; List[int]:\n    r\"\"\"Encode a tool message.\n\n    Note:\n        Same as [V3][mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_tool_message]\n        but tools are not wrapped in a list and history is also tokenized\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert message.tool_call_id is not None\n    assert isinstance(message.content, str), \"Message content must be normalized\"\n    tool_call_id_tokens = self.tokenizer.encode(message.tool_call_id, bos=False, eos=False)\n    tokens = self.tokenizer.encode(message.content, bos=False, eos=False)\n\n    prefix_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *tool_call_id_tokens,\n        self.BEGIN_TOOL_CONTENT,\n    ]\n    curr_tokens = [\n        *prefix_tokens,\n        *tokens,\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The message to encode.</p> required <code>available_tools</code> <code>Optional[List[Tool]]</code> <p>The list of available tools if any.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>is_first</code> <code>bool</code> <p>Whether the message is the first one.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Not used.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[int], List[ndarray]]</code> <p>The encoded tokens and the list of images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: Optional[List[Tool]],\n    is_last: bool,\n    is_first: bool,\n    system_prompt: Optional[str] = None,\n    force_img_first: bool = False,\n) -&gt; Tuple[List[int], List[np.ndarray]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The message to encode.\n        available_tools: The list of available tools if any.\n        is_last: Whether the message is the last one.\n        is_first: Whether the message is the first one.\n        system_prompt: Not used.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the list of images.\n    \"\"\"\n    assert system_prompt is None, \"in Tokenizer V7 we don't encode system prompts in user messages\"\n    return super().encode_user_message(\n        message,\n        available_tools,\n        is_last=is_last,\n        is_first=is_first,\n        system_prompt=None,\n        force_img_first=force_img_first,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/","title":"mistral","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral","title":"<code>mistral_common.tokens.tokenizers.mistral</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer","title":"<code>MistralTokenizer(instruct_tokenizer, validator, request_normalizer)</code>","text":"<p>               Bases: <code>Generic[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, TokenizedType]</code></p> <p>Mistral tokenizer.</p> <p>This class is a wrapper around a InstructTokenizer, a MistralRequestValidator and a InstructRequestNormalizer.</p> <p>It provides a convenient interface to tokenize, validate ad normalize Mistral requests.</p> <p>Attributes:</p> Name Type Description <code>instruct_tokenizer</code> <code>InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType]</code> <p>The instruct tokenizer to use. See InstructTokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>instruct_tokenizer</code> <code>InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType]</code> <p>The instruct tokenizer to use.</p> required <code>validator</code> <code>MistralRequestValidator[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType]</code> <p>The request validator to use.</p> required <code>request_normalizer</code> <code>InstructRequestNormalizer[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, InstructRequestType]</code> <p>The request normalizer to use.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def __init__(\n    self,\n    instruct_tokenizer: InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType],\n    validator: MistralRequestValidator[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType],\n    request_normalizer: InstructRequestNormalizer[\n        UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, InstructRequestType\n    ],\n):\n    r\"\"\"Initializes a `MistralTokenizer`.\n\n    Args:\n        instruct_tokenizer: The instruct tokenizer to use.\n        validator: The request validator to use.\n        request_normalizer: The request normalizer to use.\n    \"\"\"\n    self._chat_completion_request_validator = validator\n    self._instruct_request_normalizer = request_normalizer\n    self.instruct_tokenizer: InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType] = (\n        instruct_tokenizer\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.__reduce__","title":"<code>__reduce__()</code>","text":"<p>Provides a recipe for pickling (serializing) this object, which is necessary for use with multiprocessing.</p> <p>Returns:</p> Type Description <code>Tuple[Callable, Tuple[Any, ...]]</code> <p>A tuple of the factory function and the arguments to reconstruct the object from its source file.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def __reduce__(self) -&gt; Tuple[Callable, Tuple[Any, ...]]:\n    \"\"\"\n    Provides a recipe for pickling (serializing) this object, which is necessary for use with multiprocessing.\n\n    Returns:\n        A tuple of the factory function and the arguments to reconstruct the object from its source file.\n    \"\"\"\n    return MistralTokenizer.from_file, (\n        self.instruct_tokenizer.tokenizer.file_path,\n        self._chat_completion_request_validator._mode,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decodes a list of tokens into a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The tokens to decode.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy to use for special tokens. Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def decode(self, tokens: List[int], special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; str:\n    r\"\"\"Decodes a list of tokens into a string.\n\n    Args:\n        tokens: The tokens to decode.\n        special_token_policy: The policy to use for special tokens. Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    return self.instruct_tokenizer.decode(tokens, special_token_policy=special_token_policy)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.encode_chat_completion","title":"<code>encode_chat_completion(request, max_model_input_len=None)</code>","text":"<p>Encodes a chat completion request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest[UATS]</code> <p>The chat completion request to encode.</p> required <code>max_model_input_len</code> <code>Optional[int]</code> <p>The maximum length of the input to the model. If <code>None</code>, the input will not be truncated.</p> <code>None</code> <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The encoded chat completion request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def encode_chat_completion(\n    self, request: ChatCompletionRequest[UATS], max_model_input_len: Optional[int] = None\n) -&gt; TokenizedType:\n    r\"\"\"Encodes a chat completion request.\n\n    Args:\n        request: The chat completion request to encode.\n        max_model_input_len: The maximum length of the input to the model.\n            If `None`, the input will not be truncated.\n\n    Returns:\n        The encoded chat completion request.\n    \"\"\"\n\n    validated_request = self._chat_completion_request_validator.validate_request(request)\n\n    if max_model_input_len is None and request.truncate_for_context_length:\n        # the max_model_input_len arg should not be optional ;\n        # but this function is used in many small scripts that have no use\n        # for truncation, and don't provide the max model len\n        raise TokenizerException(\n            \"encoding a chat completion request with truncation, but no max model len was provided\",\n        )\n\n    instruct_request = self._instruct_request_normalizer.from_chat_completion_request(validated_request)\n\n    if request.truncate_for_context_length:\n        instruct_request.truncate_at_max_tokens = max_model_input_len\n\n    return self.instruct_tokenizer.encode_instruct(instruct_request)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.encode_fim","title":"<code>encode_fim(request)</code>","text":"<p>Encodes a fill in the middle request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FIMRequest</code> <p>The fill in the middle request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The encoded fill in the middle request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def encode_fim(self, request: FIMRequest) -&gt; TokenizedType:\n    r\"\"\"Encodes a fill in the middle request.\n\n    Args:\n        request: The fill in the middle request to encode.\n\n    Returns:\n        The encoded fill in the middle request.\n    \"\"\"\n    return self.instruct_tokenizer.encode_fim(request)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.from_file","title":"<code>from_file(tokenizer_filename, mode=ValidationMode.test)</code>  <code>classmethod</code>","text":"<p>Loads a tokenizer from a file.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer_filename</code> <code>Union[str, Path]</code> <p>The path to the tokenizer file.</p> required <code>mode</code> <code>ValidationMode</code> <p>The validation mode to use.</p> <code>test</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The loaded tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    tokenizer_filename: Union[str, Path],\n    mode: ValidationMode = ValidationMode.test,\n) -&gt; \"MistralTokenizer\":\n    r\"\"\"Loads a tokenizer from a file.\n\n    Args:\n        tokenizer_filename: The path to the tokenizer file.\n        mode: The validation mode to use.\n\n    Returns:\n        The loaded tokenizer.\n    \"\"\"\n    tokenizer: Union[SentencePieceTokenizer, Tekkenizer]\n\n    if is_tekken(tokenizer_filename):\n        tokenizer = Tekkenizer.from_file(tokenizer_filename)\n        image_config = tokenizer.image\n    elif is_sentencepiece(tokenizer_filename):\n        tokenizer = SentencePieceTokenizer(tokenizer_filename)\n        image_config = get_image_config(tokenizer_filename)\n    else:\n        raise TokenizerException(f\"Unrecognized tokenizer file: {tokenizer_filename}\")\n\n    image_encoder = load_image_encoder(image_config, tokenizer) if image_config is not None else None\n\n    request_normalizer = normalizer_for_tokenizer_version(tokenizer.version)\n\n    if tokenizer.version == TokenizerVersion.v1:\n        assert image_encoder is None, \"Tokenizer version needs to be &gt;= v3\"\n        return MistralTokenizer(\n            InstructTokenizerV1(tokenizer),\n            validator=MistralRequestValidator(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v2:\n        assert image_encoder is None, \"Tokenizer version needs to be &gt;= v3\"\n        return MistralTokenizer(\n            InstructTokenizerV2(tokenizer),\n            validator=MistralRequestValidator(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v3:\n        return MistralTokenizer(\n            InstructTokenizerV3(tokenizer, image_encoder=image_encoder),\n            validator=MistralRequestValidatorV3(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v7:\n        return MistralTokenizer(\n            InstructTokenizerV7(tokenizer, image_encoder=image_encoder),\n            validator=MistralRequestValidatorV5(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v11:\n        return MistralTokenizer(\n            InstructTokenizerV11(tokenizer, image_encoder=image_encoder),\n            validator=MistralRequestValidatorV5(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v13:\n        return MistralTokenizer(\n            InstructTokenizerV13(tokenizer, image_encoder=image_encoder),\n            validator=MistralRequestValidatorV13(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n\n    raise TokenizerException(f\"Unrecognized tokenizer filename: {tokenizer_filename}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.from_hf_hub","title":"<code>from_hf_hub(repo_id, token=None, revision=None, force_download=False, local_files_only=False, mode=ValidationMode.test)</code>  <code>staticmethod</code>","text":"<p>Download the Mistral tokenizer for a given Hugging Face repository ID.</p> <p>See here for a list of our OSS models.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face repo ID.</p> required <code>token</code> <code>Optional[Union[bool, str]]</code> <p>The Hugging Face token to use to download the tokenizer.</p> <code>None</code> <code>revision</code> <code>Optional[str]</code> <p>The revision of the model to use. If <code>None</code>, the latest revision will be used.</p> <code>None</code> <code>mode</code> <code>ValidationMode</code> <p>The validation mode to use.</p> <code>test</code> <code>force_download</code> <code>bool</code> <p>Whether to force the download of the tokenizer. If <code>True</code>, the tokenizer will be downloaded even if it is already cached.</p> <code>False</code> <code>local_files_only</code> <code>bool</code> <p>Whether to only use local files. If <code>True</code>, the tokenizer will be downloaded only if it is already cached.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer for the given model.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@staticmethod\ndef from_hf_hub(\n    repo_id: str,\n    token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    force_download: bool = False,\n    local_files_only: bool = False,\n    mode: ValidationMode = ValidationMode.test,\n) -&gt; \"MistralTokenizer\":\n    r\"\"\"Download the Mistral tokenizer for a given Hugging Face repository ID.\n\n    See [here](../../../../models.md#list-of-open-models) for a list of our OSS models.\n\n    Args:\n        repo_id: The Hugging Face repo ID.\n        token: The Hugging Face token to use to download the tokenizer.\n        revision: The revision of the model to use. If `None`, the latest revision will be used.\n        mode: The validation mode to use.\n        force_download: Whether to force the download of the tokenizer. If `True`, the tokenizer will be downloaded\n            even if it is already cached.\n        local_files_only: Whether to only use local files. If `True`, the tokenizer will be downloaded only if it is\n            already cached.\n\n    Returns:\n        The Mistral tokenizer for the given model.\n    \"\"\"\n    tokenizer_path = download_tokenizer_from_hf_hub(\n        repo_id=repo_id,\n        token=token,\n        revision=revision,\n        force_download=force_download,\n        local_files_only=local_files_only,\n    )\n    return MistralTokenizer.from_file(tokenizer_path, mode=mode)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.from_model","title":"<code>from_model(model, strict=False)</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model name.</p> required <code>strict</code> <code>bool</code> <p>Whether to use strict model name matching. If <code>False</code>, the model name is matched as a substring. This is deprecated and will be removed in <code>mistral_common=1.7.0</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer for the given model.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef from_model(cls, model: str, strict: bool = False) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer for a given model.\n\n    Args:\n        model: The model name.\n        strict: Whether to use strict model name matching. If `False`, the model name is matched as a substring.\n            This is deprecated and will be removed in `mistral_common=1.7.0`.\n\n    Returns:\n        The Mistral tokenizer for the given model.\n    \"\"\"\n    if not strict:\n        warnings.warn(\n            \"Calling `MistralTokenizer.from_model(..., strict=False)` is deprecated as it can lead to incorrect \"\n            \"tokenizers. It is strongly recommended to use MistralTokenizer.from_model(..., strict=True)` \"\n            \"which will become the default in `mistral_common=1.7.0`.\"\n            \"If you are using `mistral_common` for open-sourced model weights, we recommend using \"\n            \"`MistralTokenizer.from_file('&lt;path/to/tokenizer/file&gt;')` instead.\",\n            FutureWarning,\n        )\n\n        # TODO(Delete this code in mistral_common &gt;= 1.7.0\n        # Prefix search the model name mapping\n        for model_name, tokenizer_cls in MODEL_NAME_TO_TOKENIZER_CLS.items():\n            if model_name in model.lower():\n                return tokenizer_cls()\n\n    if model not in MODEL_NAME_TO_TOKENIZER_CLS:\n        raise TokenizerException(f\"Unrecognized model: {model}\")\n\n    return MODEL_NAME_TO_TOKENIZER_CLS[model]()\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v1","title":"<code>v1()</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v1.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v1(cls) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer v1.\"\"\"\n    return cls.from_file(str(cls._data_path() / \"tokenizer.model.v1\"), mode=ValidationMode.test)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v2","title":"<code>v2()</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v2.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v2(cls) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer v2.\"\"\"\n    return cls.from_file(\n        str(cls._data_path() / \"mistral_instruct_tokenizer_240216.model.v2\"), mode=ValidationMode.test\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v3","title":"<code>v3(is_tekken=False, is_mm=False)</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v3.</p> <p>Parameters:</p> Name Type Description Default <code>is_tekken</code> <code>bool</code> <p>Whether the tokenizer is a tekken tokenizer. See Tekkenizer.</p> <code>False</code> <code>is_mm</code> <code>bool</code> <p>Whether to load image tokenizer.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer v3.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v3(cls, is_tekken: bool = False, is_mm: bool = False) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer v3.\n\n    Args:\n        is_tekken: Whether the tokenizer is a tekken tokenizer. See\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer].\n        is_mm: Whether to load image tokenizer.\n\n    Returns:\n        The Mistral tokenizer v3.\n    \"\"\"\n    if is_tekken and is_mm:\n        tokenizer_name = \"tekken_240911.json\"\n    elif is_tekken and not is_mm:\n        tokenizer_name = \"tekken_240718.json\"\n    elif not is_tekken and is_mm:\n        raise ValueError(\"Multimodal tokenizer is currently only supported for tekken\")\n    else:\n        tokenizer_name = \"mistral_instruct_tokenizer_240323.model.v3\"\n\n    return cls.from_file(str(cls._data_path() / tokenizer_name), mode=ValidationMode.test)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v7","title":"<code>v7(is_mm=False)</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v7.</p> <p>Parameters:</p> Name Type Description Default <code>is_mm</code> <code>bool</code> <p>Whether to load the image tokenizer.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer v7.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v7(cls, is_mm: bool = False) -&gt; \"MistralTokenizer\":\n    \"\"\"Get the Mistral tokenizer v7.\n\n    Args:\n        is_mm: Whether to load the image tokenizer.\n\n    Returns:\n        The Mistral tokenizer v7.\n    \"\"\"\n    if is_mm:\n        return cls.from_file(\n            str(cls._data_path() / \"mistral_instruct_tokenizer_241114.model.v7m1\"), mode=ValidationMode.test\n        )\n    else:\n        return cls.from_file(\n            str(cls._data_path() / \"mistral_instruct_tokenizer_241114.model.v7\"), mode=ValidationMode.test\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.load_image_encoder","title":"<code>load_image_encoder(image_config, tokenizer)</code>","text":"<p>Load a image encoder from a config and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>image_config</code> <code>ImageConfig</code> <p>The image config.</p> required <code>tokenizer</code> <code>Union[Tekkenizer, SentencePieceTokenizer]</code> <p>The tokenizer.</p> required <p>Returns:</p> Type Description <code>ImageEncoder</code> <p>The image encoder.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def load_image_encoder(image_config: ImageConfig, tokenizer: Union[Tekkenizer, SentencePieceTokenizer]) -&gt; ImageEncoder:\n    r\"\"\"Load a image encoder from a config and a tokenizer.\n\n    Args:\n        image_config: The image config.\n        tokenizer: The tokenizer.\n\n    Returns:\n        The image encoder.\n    \"\"\"\n    special_ids = SpecialImageIDs(\n        img=tokenizer.get_control_token(SpecialTokens.img.value),\n        img_break=tokenizer.get_control_token(SpecialTokens.img_break.value),\n        img_end=tokenizer.get_control_token(SpecialTokens.img_end.value),\n    )\n    return ImageEncoder(image_config, special_ids)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/","title":"multimodal","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal","title":"<code>mistral_common.tokens.tokenizers.multimodal</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageChunk","title":"<code>ImageChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Image chunk.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>SerializableImage</code> <p>The image to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; image_chunk = ImageChunk(image=Image.new('RGB', (200, 200), color='blue'))\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: Dict[str, Union[str, Dict[str, str]]]) -&gt; \"ImageChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    assert openai_chunk.get(\"type\") == \"image_url\", openai_chunk\n\n    image_url_dict = openai_chunk[\"image_url\"]\n    assert isinstance(image_url_dict, dict) and \"url\" in image_url_dict, image_url_dict\n\n    if re.match(r\"^data:image/\\w+;base64,\", image_url_dict[\"url\"]):  # Remove the prefix if it exists\n        image_url_dict[\"url\"] = image_url_dict[\"url\"].split(\",\")[1]\n\n    return cls.model_validate({\"image\": image_url_dict[\"url\"]})\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, Dict[str, str]]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    base64_image = self.model_dump(include={\"image\"}, context={\"add_format_prefix\": True})[\"image\"]\n    return {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image}}\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageConfig","title":"<code>ImageConfig(image_patch_size, max_image_size, spatial_merge_size=1)</code>  <code>dataclass</code>","text":"<p>Configuration for the image tokenizers.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageEncoder","title":"<code>ImageEncoder(image_config, special_ids)</code>","text":"<p>Image encoder for the image tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>image_config</code> <code>ImageConfig</code> <p>Configuration for the image tokenizer.</p> required <code>special_ids</code> <code>SpecialImageIDs</code> <p>Special image tokens ids.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def __init__(self, image_config: ImageConfig, special_ids: SpecialImageIDs) -&gt; None:\n    r\"\"\"Initialize the image encoder.\n\n    Args:\n        image_config: Configuration for the image tokenizer.\n        special_ids: Special image tokens ids.\n    \"\"\"\n    self.image_config = image_config\n    self.special_ids = special_ids\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageEncoder.__call__","title":"<code>__call__(content)</code>","text":"<p>Converts an image chunk to an image encoding.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[ImageChunk, ImageURLChunk]</code> <p>image chunk to be converted.</p> required <p>Returns:</p> Type Description <code>ImageEncoding</code> <p>Image encoding.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def __call__(self, content: Union[ImageChunk, ImageURLChunk]) -&gt; ImageEncoding:\n    r\"\"\"Converts an image chunk to an image encoding.\n\n    Args:\n        content: image chunk to be converted.\n\n    Returns:\n        Image encoding.\n    \"\"\"\n    image = image_from_chunk(content)\n    w, h = self._image_to_num_tokens(image)\n    assert w &gt; 0\n    assert h &gt; 0\n    image_tokens = ([self.special_ids.img] * w + [self.special_ids.img_break]) * h\n    image_tokens[-1] = self.special_ids.img_end\n    new_image_size = (\n        w * self.image_config.image_patch_size * self.image_config.spatial_merge_size,\n        h * self.image_config.image_patch_size * self.image_config.spatial_merge_size,\n    )\n    processed_image = transform_image(image, new_image_size)\n    return ImageEncoding(tokens=image_tokens, image=processed_image)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageEncoding","title":"<code>ImageEncoding(tokens, image)</code>  <code>dataclass</code>","text":"<p>A tokenized image.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>List[int]</code> <p>The token ids.</p> <code>image</code> <code>ndarray</code> <p>The image as a numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; image_encoding = ImageEncoding(tokens=[1, 2, 3], image=np.array([[0., 0.5, 1.]]))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageURLChunk","title":"<code>ImageURLChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Image URL chunk.</p> <p>Attributes:</p> Name Type Description <code>image_url</code> <code>Union[ImageURL, str]</code> <p>The URL of the image or a base64 encoded image to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_url_chunk = ImageURLChunk(image_url=\"data:image/png;base64,iVBORw0\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageURLChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: Dict[str, Union[str, Dict[str, str]]]) -&gt; \"ImageURLChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate({\"image_url\": openai_chunk[\"image_url\"]})\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.ImageURLChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; Dict[str, Union[str, Dict[str, str]]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    image_url_dict = {\"url\": self.get_url()}\n    if isinstance(self.image_url, ImageURL) and self.image_url.detail is not None:\n        image_url_dict[\"detail\"] = self.image_url.detail\n\n    out_dict: Dict[str, Union[str, Dict[str, str]]] = {\n        \"type\": \"image_url\",\n        \"image_url\": image_url_dict,\n    }\n    return out_dict\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.MultiModalVersion","title":"<code>MultiModalVersion</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Version of the image tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.SpecialImageIDs","title":"<code>SpecialImageIDs(img, img_break, img_end)</code>  <code>dataclass</code>","text":"<p>Special image tokens ids.</p> <p>Attributes:</p> Name Type Description <code>img</code> <code>int</code> <p>The image token id.</p> <code>img_break</code> <code>int</code> <p>The image break token id.</p> <code>img_end</code> <code>int</code> <p>The image end token id.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; special_image_ids = SpecialImageIDs(img=1, img_break=2, img_end=3)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.download_image","title":"<code>download_image(url)</code>","text":"<p>Download an image from a URL and return it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image to download.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The downloaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def download_image(url: str) -&gt; Image.Image:\n    r\"\"\"Download an image from a URL and return it as a PIL Image.\n\n    Args:\n        url: The URL of the image to download.\n\n    Returns:\n       The downloaded image as a PIL Image object.\n    \"\"\"\n    headers = {\"User-Agent\": f\"mistral-common/{__version__}\"}\n    try:\n        # Make a request to download the image\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n\n        # Convert the image content to a PIL Image\n        img = Image.open(io.BytesIO(response.content))\n        return img\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error downloading the image from {url}: {e}.\")\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to PIL image: {e}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.image_from_chunk","title":"<code>image_from_chunk(chunk)</code>","text":"<p>Get a serializable image from a chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>Union[ImageURLChunk, ImageChunk]</code> <p>The chunk to get the image from.</p> required <p>Returns:</p> Type Description <code>SerializableImage</code> <p>The image as a PIL Image object.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def image_from_chunk(chunk: Union[ImageURLChunk, ImageChunk]) -&gt; SerializableImage:\n    r\"\"\"Get a serializable image from a chunk.\n\n    Args:\n        chunk: The chunk to get the image from.\n\n    Returns:\n        The image as a PIL Image object.\n    \"\"\"\n    if isinstance(chunk, ImageChunk):\n        return chunk.image\n    if chunk.get_url().startswith(\"data:image\"):\n        data = chunk.get_url().split(\",\")[1]\n        image_data = base64.b64decode(data)\n        return Image.open(BytesIO(image_data))\n    if chunk.get_url().startswith(\"file\"):\n        return Image.open(open(chunk.get_url().replace(\"file://\", \"\"), \"rb\"))\n    if chunk.get_url().startswith(\"http\"):\n        return download_image(chunk.get_url())\n\n    raise RuntimeError(f\"Unsupported image url scheme {chunk.get_url()}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.is_cv2_installed","title":"<code>is_cv2_installed()</code>","text":"<p>Check if OpenCV is installed.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def is_cv2_installed() -&gt; bool:\n    r\"\"\"Check if OpenCV is installed.\"\"\"\n    return _cv2_installed\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.normalize","title":"<code>normalize(np_image, mean, std)</code>","text":"<p>Normalize a tensor image with mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>np_image</code> <code>ndarray</code> <p>Image to be normalized.</p> required <code>mean</code> <code>Tuple[float, float, float]</code> <p>Mean for each channel.</p> required <code>std</code> <code>Tuple[float, float, float]</code> <p>Standard deviation for each channel.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized image with shape (C, H, W).</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def normalize(\n    np_image: np.ndarray,\n    mean: Tuple[float, float, float],\n    std: Tuple[float, float, float],\n) -&gt; np.ndarray:\n    r\"\"\"Normalize a tensor image with mean and standard deviation.\n\n    Args:\n        np_image: Image to be normalized.\n        mean: Mean for each channel.\n        std: Standard deviation for each channel.\n\n    Returns:\n        Normalized image with shape (C, H, W).\n    \"\"\"\n    np_image = np_image / 255.0\n\n    assert len(np_image.shape) == 3, f\"{np_image.shape=}\"\n    assert np_image.shape[2] == len(mean) == len(std), f\"{np_image.shape=}, {mean=}, {std=}\"\n\n    np_image = (np_image - mean) / std\n\n    return np_image.transpose(2, 0, 1)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/multimodal/#mistral_common.tokens.tokenizers.multimodal.transform_image","title":"<code>transform_image(image, new_size)</code>","text":"<p>Transform an image to a numpy array with the given size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image to be transformed.</p> required <code>new_size</code> <code>Tuple[int, int]</code> <p>New size of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Transformed image with shape (C, H, W).</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def transform_image(image: Image.Image, new_size: Tuple[int, int]) -&gt; np.ndarray:\n    r\"\"\"Transform an image to a numpy array with the given size.\n\n    Args:\n        image: Image to be transformed.\n        new_size: New size of the image.\n\n    Returns:\n        Transformed image with shape (C, H, W).\n    \"\"\"\n    if not is_cv2_installed():\n        raise ImportError(\"OpenCV is required for this function. Install it with 'pip install mistral-common[opencv]'\")\n\n    np_image = cv2.resize(np.array(_convert_to_rgb(image), dtype=np.float32), new_size, interpolation=cv2.INTER_CUBIC)\n    return normalize(np_image, DATASET_MEAN, DATASET_STD)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/","title":"sentencepiece","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece","title":"<code>mistral_common.tokens.tokenizers.sentencepiece</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer","title":"<code>SentencePieceTokenizer(model_path, tokenizer_version=None)</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>SentencePiece tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Union[str, Path]</code> <p>The path to the <code>SentencePiece</code> model.</p> required <code>tokenizer_version</code> <code>Optional[TokenizerVersion]</code> <p>The version of the tokenizer. If not provided, it will be inferred from the model path.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def __init__(self, model_path: Union[str, Path], tokenizer_version: Optional[TokenizerVersion] = None) -&gt; None:\n    r\"\"\"Initialize the `SentencePieceTokenizer`.\n\n    Args:\n        model_path: The path to the `SentencePiece` model.\n        tokenizer_version: The version of the tokenizer. If not provided, it will be inferred from the model path.\n    \"\"\"\n    self._logger = logging.getLogger(self.__class__.__name__)\n    # reload tokenizer\n    assert os.path.isfile(model_path), model_path\n    self._model = SentencePieceProcessor(\n        model_file=model_path if isinstance(model_path, str) else model_path.as_posix()\n    )\n\n    assert self._model.vocab_size() == self._model.get_piece_size()\n    self._vocab = [self._model.id_to_piece(i) for i in range(self.n_words)]\n\n    self._version: TokenizerVersion = tokenizer_version or get_spm_version(model_path, raise_deprecated=False)\n\n    self._file_path = Path(model_path)\n    super().__init__()\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.bos_id","title":"<code>bos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The beginning of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.eos_id","title":"<code>eos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The end of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>The path to the tokenizer model.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.n_words","title":"<code>n_words</code>  <code>property</code>","text":"<p>Vocabulary size of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.pad_id","title":"<code>pad_id</code>  <code>property</code>","text":"<p>The padding token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.unk_id","title":"<code>unk_id</code>  <code>property</code>","text":"<p>The unknown token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.version","title":"<code>version</code>  <code>property</code>","text":"<p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decode the given list of token ids into a string.</p> Note <p>Using <code>special_token_policy=SpecialTokenPolicy.KEEP</code> will keep the special tokens and the normal tokens as SentencePiece pieces.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The list of token ids.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy to use for special tokens. If <code>None</code>, the default policy is <code>SpecialTokenPolicy.IGNORE</code>.  Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def decode(self, tokens: List[int], special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; str:\n    r\"\"\"Decode the given list of token ids into a string.\n\n    Note:\n        Using `special_token_policy=SpecialTokenPolicy.KEEP` will keep the special tokens and the normal tokens as\n        SentencePiece pieces.\n\n    Args:\n        tokens: The list of token ids.\n        special_token_policy: The policy to use for special tokens. If `None`, the default policy\n            is `SpecialTokenPolicy.IGNORE`.  Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    if special_token_policy is not None and not isinstance(special_token_policy, SpecialTokenPolicy):\n        raise ValueError(\n            f\"Expected `special_token_policy` to be None or SpecialTokenPolicy, got {type(special_token_policy)}.\"\n        )\n\n    if special_token_policy is None:\n        warnings.warn(\n            (\n                \"Using the tokenizer's special token policy `None` is deprecated. \"\n                \"It will be removed in 1.7.0. \"\n                \"Please pass a special token policy explicitly. \"\n                \"Future default will be SpecialTokenPolicy.IGNORE.\"\n            ),\n            FutureWarning,\n        )\n        special_token_policy = SpecialTokenPolicy.IGNORE\n\n    if special_token_policy in [SpecialTokenPolicy.KEEP, SpecialTokenPolicy.RAISE]:\n        return self._decode_with_special_tokens(tokens, special_token_policy)\n\n    return self._model.decode(tokens)  # type: ignore\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.encode","title":"<code>encode(s, bos, eos)</code>","text":"<p>Encode the given string into a list of token ids.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to encode.</p> required <code>bos</code> <code>bool</code> <p>Whether to add the beginning of sentence token.</p> required <code>eos</code> <code>bool</code> <p>Whether to add the end of sentence token.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The list of token ids.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def encode(self, s: str, bos: bool, eos: bool) -&gt; List[int]:\n    r\"\"\"Encode the given string into a list of token ids.\n\n    Args:\n        s: The string to encode.\n        bos: Whether to add the beginning of sentence token.\n        eos: Whether to add the end of sentence token.\n\n    Returns:\n        The list of token ids.\n    \"\"\"\n    assert isinstance(s, str)\n    t: List[int] = self._model.encode(s)\n    if bos:\n        t = [self.bos_id, *t]\n    if eos:\n        t = [*t, self.eos_id]\n    return t\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.get_control_token","title":"<code>get_control_token(s)</code>","text":"<p>Get the control token for the given string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def get_control_token(self, s: str) -&gt; int:\n    r\"\"\"Get the control token for the given string.\"\"\"\n    return self._model.piece_to_id(s)  # type: ignore\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.id_to_piece","title":"<code>id_to_piece(token_id)</code>","text":"<p>Convert the given token id to a token piece.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def id_to_piece(self, token_id: int) -&gt; str:\n    r\"\"\"Convert the given token id to a token piece.\"\"\"\n    return self._model.id_to_piece(token_id)  # type: ignore\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.to_string","title":"<code>to_string(tokens)</code>","text":"<p>[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.</p> <p>Use <code>decode</code> with <code>special_token_policy=SpecialTokenPolicy.KEEP</code> instead.</p> <p>This is a convenient method for debugging.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def to_string(self, tokens: List[int]) -&gt; str:\n    r\"\"\"[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.\n\n    Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\n\n    This is a convenient method for debugging.\n    \"\"\"\n    warnings.warn(\n        (\n            \"`to_string` is deprecated and will be removed in 1.7.0. \"\n            \"Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\"\n        ),\n        FutureWarning,\n    )\n    return self._to_string(tokens)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.vocab","title":"<code>vocab()</code>","text":"<p>All tokens in the vocabulary as strings.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def vocab(self) -&gt; List[str]:\n    r\"\"\"All tokens in the vocabulary as strings.\"\"\"\n    return self._vocab\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.get_image_config","title":"<code>get_image_config(tokenizer_filename)</code>","text":"<p>Get the image config from the tokenizer filename.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def get_image_config(tokenizer_filename: Union[str, Path]) -&gt; Optional[ImageConfig]:\n    r\"\"\"Get the image config from the tokenizer filename.\"\"\"\n    tokenizer_filename = str(tokenizer_filename)\n\n    _version_str = tokenizer_filename.split(\".\")[-1]\n    if _version_str == \"model\" or \"m\" not in _version_str:\n        return None\n\n    _mm_version_str = \"m\" + _version_str.split(\"m\")[-1]\n\n    if _mm_version_str not in MultiModalVersion.__members__:\n        raise TokenizerException(f\"Unrecognized tokenizer filename: {tokenizer_filename}\")\n\n    return MultiModalVersion(_mm_version_str).config\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.get_spm_version","title":"<code>get_spm_version(tokenizer_filename, raise_deprecated=False)</code>","text":"<p>Get the version of the tokenizer from the filename.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def get_spm_version(tokenizer_filename: Union[str, Path], raise_deprecated: bool = False) -&gt; TokenizerVersion:\n    r\"\"\"Get the version of the tokenizer from the filename.\"\"\"\n    tokenizer_filename = str(tokenizer_filename)\n\n    _version_str = tokenizer_filename.split(\".\")[-1]\n    if _version_str != \"model\":  # filter tokenizer_filename == \"/path/to/tokenizer.model\" case\n        _version_str = _version_str.split(\"m\")[0]\n\n    if _version_str == \"model\":\n        if raise_deprecated:\n            raise TokenizerException(f\"Make sure to rename your tokenizer file to end with {tokenizer_filename}.v1.\")\n\n        # tokenizer.model =&gt; tokenizer.model.v1\n        return TokenizerVersion(\"v1\")\n\n    if _version_str not in TokenizerVersion.__members__:\n        raise TokenizerException(f\"Unrecognized tokenizer filename: {tokenizer_filename}\")\n\n    return TokenizerVersion(_version_str)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.is_sentencepiece","title":"<code>is_sentencepiece(path)</code>","text":"<p>Check if the given path is a SentencePiece model.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def is_sentencepiece(path: Union[str, Path]) -&gt; bool:\n    r\"\"\"Check if the given path is a SentencePiece model.\"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n\n    instruct_versions = list(TokenizerVersion.__members__)\n    mm_versions = list(MultiModalVersion.__members__) + [\"\"]  # allow no mm version\n    suffixes = [f\".model.{v}{m}\" for v in instruct_versions for m in mm_versions] + [\".model\"]\n\n    return path.is_file() and any(path.name.endswith(suffix) for suffix in suffixes)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/","title":"tekken","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken","title":"<code>mistral_common.tokens.tokenizers.tekken</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.ModelData","title":"<code>ModelData</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The data of the tekken tokenizer model.</p> <p>Attributes:</p> Name Type Description <code>vocab</code> <code>List[TokenInfo]</code> <p>The vocabulary of the tokenizer.</p> <code>config</code> <code>TekkenConfig</code> <p>The configuration of the tokenizer.</p> <code>version</code> <code>int</code> <p>The version of the tokenizer.</p> <code>type</code> <code>str</code> <p>The type of the tokenizer.</p> <code>image</code> <code>ImageConfig</code> <p>The image configuration of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.SpecialTokenInfo","title":"<code>SpecialTokenInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Special token information in the JSON file.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>The rank of the token.</p> <code>token_str</code> <code>str</code> <p>The token in string format.</p> <code>is_control</code> <code>bool</code> <p>Whether the token is a control token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.TekkenConfig","title":"<code>TekkenConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tekken configuration in the JSON file.</p> <p>Attributes:</p> Name Type Description <code>pattern</code> <code>str</code> <p>The pattern of the tokenizer.</p> <code>num_vocab_tokens</code> <code>int</code> <p>The number of vocabulary tokens.</p> <code>default_vocab_size</code> <code>int</code> <p>The default vocabulary size.</p> <code>default_num_special_tokens</code> <code>int</code> <p>The default number of special tokens.</p> <code>version</code> <code>str</code> <p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer","title":"<code>Tekkenizer(vocab, special_tokens, pattern, vocab_size, num_special_tokens, version, *, name='tekkenizer', _path=None, image_config=None)</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Tekken tokenizer.</p> <p>This tokenizer is based on the tiktoken library. It fastens the tokenization for multiple languages.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>List[TokenInfo]</code> <p>The vocabulary of the tokenizer.</p> required <code>special_tokens</code> <code>List[SpecialTokenInfo]</code> <p>The special tokens of the tokenizer.</p> required <code>pattern</code> <code>str</code> <p>The pattern of the tokenizer.</p> required <code>vocab_size</code> <code>int</code> <p>The vocabulary size of the tokenizer.</p> required <code>num_special_tokens</code> <code>int</code> <p>The number of special tokens of the tokenizer.</p> required <code>version</code> <code>TokenizerVersion</code> <p>The version of the tokenizer.</p> required <code>name</code> <code>str</code> <p>The name of the tokenizer.</p> <code>'tekkenizer'</code> <code>image_config</code> <code>Optional[ImageConfig]</code> <p>The image configuration of the tokenizer.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def __init__(\n    self,\n    vocab: List[TokenInfo],\n    special_tokens: List[SpecialTokenInfo],\n    pattern: str,\n    vocab_size: int,\n    num_special_tokens: int,\n    version: TokenizerVersion,\n    *,\n    name: str = \"tekkenizer\",\n    _path: Optional[Union[str, Path]] = None,\n    image_config: Optional[ImageConfig] = None,\n):\n    r\"\"\"Initialize the tekken tokenizer.\n\n    Args:\n        vocab: The vocabulary of the tokenizer.\n        special_tokens: The special tokens of the tokenizer.\n        pattern: The pattern of the tokenizer.\n        vocab_size: The vocabulary size of the tokenizer.\n        num_special_tokens: The number of special tokens of the tokenizer.\n        version: The version of the tokenizer.\n        name: The name of the tokenizer.\n        image_config: The image configuration of the tokenizer.\n    \"\"\"\n    assert vocab_size &lt;= len(vocab) + num_special_tokens, (\n        vocab_size,\n        len(vocab),\n        num_special_tokens,\n    )\n    self._vocab_size = vocab_size\n\n    # The number of special tokens defined in the tokenizer json\n    num_defined_special_tokens = len(set([t[\"token_str\"] for t in special_tokens]))\n\n    assert len(special_tokens) == num_defined_special_tokens, f\"Special tokens must be unique: {special_tokens}\"\n    assert len(special_tokens) &lt;= num_special_tokens\n\n    special_filler = [\n        SpecialTokenInfo(rank=i, token_str=self.SPECIAL_TOKEN_TEMPLATE.format(id=i), is_control=True)\n        for i in range(len(special_tokens), num_special_tokens)\n    ]\n    if special_filler:\n        logger.info(\n            f\"Adding special tokens {special_filler[0]['token_str']}, ..., {special_filler[-1]['token_str']}\"\n        )\n    special_tokens = special_tokens + special_filler\n\n    assert len(set([t[\"token_str\"] for t in special_tokens])) == len(special_tokens) == num_special_tokens, (\n        special_tokens\n    )\n    inner_vocab_size = vocab_size - num_special_tokens\n\n    # reload vocab\n    self._tekken_token2id_nospecial = _reload_mergeable_ranks(vocab, max_vocab=inner_vocab_size)\n    assert set(range(inner_vocab_size)) == set(self._tekken_token2id_nospecial.values()), (\n        inner_vocab_size,\n        self._tekken_token2id_nospecial,\n    )\n    self._model = tiktoken.Encoding(\n        name=name,\n        pat_str=pattern,\n        mergeable_ranks=self._tekken_token2id_nospecial,\n        special_tokens={},  # special tokens are handled manually\n    )\n\n    self._version = version\n    self._image_config = image_config\n    self._all_special_tokens = special_tokens\n    self._special_tokens_reverse_vocab = {t[\"token_str\"]: t[\"rank\"] for t in special_tokens}\n    self._vocab = [self.id_to_piece(i) for i in range(vocab_size)]\n    self._special_token_policy = SpecialTokenPolicy.IGNORE\n    self._file_path = Path(_path) if _path is not None else None\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.bos_id","title":"<code>bos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The beginning of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.eos_id","title":"<code>eos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The end of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>The path to the tokenizer file.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.image","title":"<code>image</code>  <code>property</code> <code>writable</code>","text":"<p>The image configuration of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.n_words","title":"<code>n_words</code>  <code>property</code>","text":"<p>Vocabulary size of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.num_special_tokens","title":"<code>num_special_tokens</code>  <code>property</code>","text":"<p>The number of special tokens of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.pad_id","title":"<code>pad_id</code>  <code>cached</code> <code>property</code>","text":"<p>The padding token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_token_policy","title":"<code>special_token_policy</code>  <code>property</code> <code>writable</code>","text":"<p>The policy for handling special tokens.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.unk_id","title":"<code>unk_id</code>  <code>cached</code> <code>property</code>","text":"<p>The unknown token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.version","title":"<code>version</code>  <code>property</code>","text":"<p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decode a list of token ids into a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The list of token ids to decode.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy for handling special tokens. Use the tokenizer's attribute if <code>None</code>. Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def decode(self, tokens: List[int], special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; str:\n    r\"\"\"Decode a list of token ids into a string.\n\n    Args:\n        tokens: The list of token ids to decode.\n        special_token_policy: The policy for handling special tokens.\n            Use the tokenizer's [attribute][mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_token_policy]\n            if `None`. Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    if special_token_policy is not None and not isinstance(special_token_policy, SpecialTokenPolicy):\n        raise ValueError(\n            f\"Expected `special_token_policy` to be None or SpecialTokenPolicy, got {type(special_token_policy)}.\"\n        )\n\n    if special_token_policy is None:\n        warnings.warn(\n            (\n                f\"Using the tokenizer's special token policy ({self._special_token_policy}) is deprecated. \"\n                \"It will be removed in 1.7.0. \"\n                \"Please pass a special token policy explicitly. \"\n                \"Future default will be SpecialTokenPolicy.IGNORE.\"\n            ),\n            FutureWarning,\n        )\n        special_token_policy = self._special_token_policy\n\n    return \"\".join(self._decode_all(tokens, special_token_policy=special_token_policy))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.encode","title":"<code>encode(s, bos, eos)</code>","text":"<p>Encode a string into a list of token ids.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to encode.</p> required <code>bos</code> <code>bool</code> <p>Whether to add the beginning of sentence token.</p> required <code>eos</code> <code>bool</code> <p>Whether to add the end of sentence token.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The list of token ids.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def encode(self, s: str, bos: bool, eos: bool) -&gt; List[int]:\n    r\"\"\"Encode a string into a list of token ids.\n\n    Args:\n        s: The string to encode.\n        bos: Whether to add the beginning of sentence token.\n        eos: Whether to add the end of sentence token.\n\n    Returns:\n        The list of token ids.\n    \"\"\"\n    tokens: List[int] = self._model.encode(s)\n    tokens = [t + self.num_special_tokens for t in tokens]\n    if bos:\n        tokens = [self.bos_id, *tokens]\n    if eos:\n        tokens = [*tokens, self.eos_id]\n    return tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Load the tekken tokenizer from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The path to the tokenizer file.</p> required <p>Returns:</p> Type Description <code>Tekkenizer</code> <p>The tekken tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>@classmethod\ndef from_file(cls: Type[\"Tekkenizer\"], path: Union[str, Path]) -&gt; \"Tekkenizer\":\n    r\"\"\"Load the tekken tokenizer from a file.\n\n    Args:\n        path: The path to the tokenizer file.\n\n    Returns:\n        The tekken tokenizer.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n    assert path.exists(), path\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        untyped = json.load(f)\n\n    _version_str = untyped[\"config\"].get(\"version\")\n    if _version_str not in TokenizerVersion.__members__:\n        raise ValueError(\n            f\"Unknown version: {_version_str} in {path}. \"\n            f\"Make sure to use a valid version string: {list(TokenizerVersion.__members__)}\"\n        )\n\n    assert _version_str is not None\n    version = TokenizerVersion(_version_str)\n\n    special_tokens_dicts: Optional[List[SpecialTokenInfo]] = untyped.get(\"special_tokens\", None)\n    if special_tokens_dicts is None:\n        # Tokenizer &gt; v7 should find special tokens in the tokenizer file\n        if version &gt; TokenizerVersion(\"v7\"):\n            raise ValueError(\n                f\"Special tokens not found in {path}. \"\n                \"Please update your tokenizer file and include all special tokens you need.\"\n            )\n        else:\n            special_tokens = list(Tekkenizer.DEPRECATED_SPECIAL_TOKENS)\n    else:\n        special_tokens = [token for token in special_tokens_dicts]\n\n    untyped[\"special_tokens\"] = special_tokens\n\n    if mm := untyped.get(\"multimodal\", None):\n        # deprecated - only allowed for tokenizers &lt;= v11\n        if version &gt; TokenizerVersion(\"v11\"):\n            raise ValueError(\n                f\"The image config has to be called 'image' in {path} for tokenizers of version {version.value}.\"\n            )\n\n        untyped[\"image\"] = ImageConfig(**mm)\n    elif image := untyped.get(\"image\", None):\n        untyped[\"image\"] = ImageConfig(**image)\n\n    model_data: ModelData = untyped\n\n    return cls(\n        vocab=model_data[\"vocab\"],\n        special_tokens=special_tokens,\n        pattern=model_data[\"config\"][\"pattern\"],\n        vocab_size=model_data[\"config\"][\"default_vocab_size\"],\n        num_special_tokens=model_data[\"config\"][\"default_num_special_tokens\"],\n        version=version,\n        name=path.name.replace(\".json\", \"\"),\n        image_config=model_data.get(\"image\"),\n        _path=path,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.get_control_token","title":"<code>get_control_token(s)</code>","text":"<p>Get the token id of a control token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def get_control_token(self, s: str) -&gt; int:\n    r\"\"\"Get the token id of a control token.\"\"\"\n    if s in self._special_tokens_reverse_vocab:\n        return self._special_tokens_reverse_vocab[s]\n    else:\n        raise ValueError(f\"Unknown control token {s}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.id_to_byte_piece","title":"<code>id_to_byte_piece(token_id, special_token_policy=None)</code>","text":"<p>Convert a token id to its byte representation.</p> <p>Parameters:</p> Name Type Description Default <code>token_id</code> <code>int</code> <p>The token id to convert.</p> required <code>special_token_policy</code> <code>Optional[SpecialTokenPolicy]</code> <p>The policy for handling special tokens. Use the tokenizer's attribute if <code>None</code>. Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.7.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The byte representation of the token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def id_to_byte_piece(self, token_id: int, special_token_policy: Optional[SpecialTokenPolicy] = None) -&gt; bytes:\n    r\"\"\"Convert a token id to its byte representation.\n\n    Args:\n        token_id: The token id to convert.\n        special_token_policy: The policy for handling special tokens.\n            Use the tokenizer's [attribute][mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_token_policy]\n            if `None`. Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.7.0`.\n\n    Returns:\n        The byte representation of the token.\n    \"\"\"\n    if special_token_policy is None:\n        warnings.warn(\n            (\n                f\"Using the tokenizer's special token policy ({self._special_token_policy}) is deprecated. \"\n                \"It will be removed in 1.7.0. \"\n                \"Please pass a special token policy explicitly. \"\n                \"Future default will be SpecialTokenPolicy.IGNORE.\"\n            ),\n            FutureWarning,\n        )\n        special_token_policy = self._special_token_policy\n\n    if token_id &lt; self.num_special_tokens:\n        if special_token_policy == SpecialTokenPolicy.KEEP:\n            return self._all_special_tokens[token_id][\"token_str\"].encode(\"utf-8\")\n        elif special_token_policy == SpecialTokenPolicy.RAISE:\n            raise ValueError(f\"{token_id} is a special token\")\n        elif special_token_policy == SpecialTokenPolicy.IGNORE:\n            return b\"\"\n        else:\n            raise ValueError(f\"Unknown special token policy {special_token_policy}\")\n\n    return self._model.decode_single_token_bytes(token_id - self.num_special_tokens)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.id_to_piece","title":"<code>id_to_piece(token_id)</code>","text":"<p>Convert a token id to its string representation.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def id_to_piece(self, token_id: int) -&gt; str:\n    r\"\"\"Convert a token id to its string representation.\"\"\"\n    return self.decode([token_id], special_token_policy=SpecialTokenPolicy.KEEP)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.is_byte","title":"<code>is_byte(token_id)</code>","text":"<p>Check if a token id is a byte token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def is_byte(self, token_id: int) -&gt; bool:\n    r\"\"\"Check if a token id is a byte token.\"\"\"\n    return 0 &lt;= token_id - self.num_special_tokens &lt; 256\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.to_string","title":"<code>to_string(tokens)</code>","text":"<p>[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.</p> <p>Use <code>decode</code> with <code>special_token_policy=SpecialTokenPolicy.KEEP</code> instead.</p> <p>This is a convenient method for debugging.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def to_string(self, tokens: List[int]) -&gt; str:\n    r\"\"\"[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.\n\n    Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\n\n    This is a convenient method for debugging.\n    \"\"\"\n    warnings.warn(\n        (\n            \"`to_string` is deprecated and will be removed in 1.7.0. \"\n            \"Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\"\n        ),\n        FutureWarning,\n    )\n    return self._to_string(tokens)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.vocab","title":"<code>vocab()</code>","text":"<p>All tokens in the vocabulary as strings.</p> Note <p>This will collapse all tokens for which we have a decoding error into the &lt;?&gt; string. This is bad and results in things like len(set(vocab)) != len(vocab)).</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The vocabulary of the tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def vocab(self) -&gt; List[str]:\n    r\"\"\"All tokens in the vocabulary as strings.\n\n    Note:\n       This will collapse all tokens for which we have a decoding error into\n       the &lt;?&gt; string. This is bad and results in things like len(set(vocab)) != len(vocab)).\n\n    Returns:\n        The vocabulary of the tokenizer.\n    \"\"\"\n    # when returning self._vocab this will collapse\n    # all tokens for which we have a decoding error into\n    # the &lt;?&gt; string. This is bad and results in things\n    # like len(set(vocab)) != len(vocab))\n    # be careful when using self._vocab\n    return self._vocab\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.TokenInfo","title":"<code>TokenInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Token information in the JSON file.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>The rank of the token.</p> <code>token_bytes</code> <code>str</code> <p>The token in bytes, base64 encoded.</p> <code>token_str</code> <code>Optional[str]</code> <p>The token in string format.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.is_tekken","title":"<code>is_tekken(path)</code>","text":"<p>Check if the given path is a tekken tokenizer file.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def is_tekken(path: Union[str, Path]) -&gt; bool:\n    r\"\"\"Check if the given path is a tekken tokenizer file.\"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n    return path.is_file() and \"tekken\" in path.name and path.suffix == \".json\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/","title":"utils","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils","title":"<code>mistral_common.tokens.tokenizers.utils</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.chunks","title":"<code>chunks(lst, chunk_size)</code>","text":"<p>Chunk a list into smaller lists of a given size.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>List[str]</code> <p>The list to chunk.</p> required <code>chunk_size</code> <code>int</code> <p>The size of each chunk.</p> required <p>Returns:</p> Type Description <code>Iterator[List[str]]</code> <p>An iterator over the chunks.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; all_chunks = list(chunks([1, 2, 3, 4, 5], 2))\n</code></pre> Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def chunks(lst: List[str], chunk_size: int) -&gt; Iterator[List[str]]:\n    r\"\"\"Chunk a list into smaller lists of a given size.\n\n    Args:\n        lst: The list to chunk.\n        chunk_size: The size of each chunk.\n\n    Returns:\n        An iterator over the chunks.\n\n    Examples:\n        &gt;&gt;&gt; all_chunks = list(chunks([1, 2, 3, 4, 5], 2))\n    \"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i : i + chunk_size]\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.download_tokenizer_from_hf_hub","title":"<code>download_tokenizer_from_hf_hub(repo_id, cache_dir=None, token=None, revision=None, force_download=False, local_files_only=False)</code>","text":"<p>Download the tokenizer file of a Mistral model from the Hugging Face Hub.</p> <p>See here for a list of our OSS models.</p> Note <p>You need to install the <code>huggingface_hub</code> package to use this method.</p> <p>Please run <code>pip install mistral-common[hf-hub]</code> to install it.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face repo ID.</p> required <code>cache_dir</code> <code>Optional[Union[str, Path]]</code> <p>The directory where the tokenizer will be cached.</p> <code>None</code> <code>token</code> <code>Optional[Union[bool, str]]</code> <p>The Hugging Face token to use to download the tokenizer.</p> <code>None</code> <code>revision</code> <code>Optional[str]</code> <p>The revision of the model to use. If <code>None</code>, the latest revision will be used.</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>Whether to force the download of the tokenizer. If <code>True</code>, the tokenizer will be downloaded even if it is already cached.</p> <code>False</code> <code>local_files_only</code> <code>bool</code> <p>Whether to only use local files. If <code>True</code>, the tokenizer will be downloaded only if it is already cached.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The downloaded tokenizer local path for the given model ID.</p> Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def download_tokenizer_from_hf_hub(\n    repo_id: str,\n    cache_dir: Optional[Union[str, Path]] = None,\n    token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    force_download: bool = False,\n    local_files_only: bool = False,\n) -&gt; str:\n    r\"\"\"Download the tokenizer file of a Mistral model from the Hugging Face Hub.\n\n    See [here](../../../../models.md#list-of-open-models) for a list of our OSS models.\n\n    Note:\n        You need to install the `huggingface_hub` package to use this method.\n\n        Please run `pip install mistral-common[hf-hub]` to install it.\n\n    Args:\n        repo_id: The Hugging Face repo ID.\n        cache_dir: The directory where the tokenizer will be cached.\n        token: The Hugging Face token to use to download the tokenizer.\n        revision: The revision of the model to use. If `None`, the latest revision will be used.\n        force_download: Whether to force the download of the tokenizer. If `True`, the tokenizer will be downloaded\n            even if it is already cached.\n        local_files_only: Whether to only use local files. If `True`, the tokenizer will be downloaded only if it is\n            already cached.\n\n    Returns:\n        The downloaded tokenizer local path for the given model ID.\n    \"\"\"\n    _assert_hub_installed()\n\n    if force_download and local_files_only:\n        raise ValueError(\"You cannot force the download of the tokenizer if you only want to use local files.\")\n\n    if not local_files_only:\n        try:\n            hf_api = huggingface_hub.HfApi()\n            repo_files = hf_api.list_repo_files(repo_id)\n            local_files_only = False\n        except (requests.ConnectionError, requests.HTTPError, requests.Timeout) as e:\n            if force_download:\n                raise e\n\n            repo_files = list_local_hf_repo_files(repo_id=repo_id, revision=revision)\n            local_files_only = True\n\n            logger.info(\"Could not connect to the Hugging Face Hub. Using local files only.\")\n\n            if len(repo_files) == 0:\n                raise FileNotFoundError(\n                    f\"Could not connect to the Hugging Face Hub and no local files were found for the repo ID {repo_id}\"\n                    f\" and revision {revision}. Please check your internet connection and try again.\"\n                ) from e\n    else:\n        repo_files = list_local_hf_repo_files(repo_id=repo_id, revision=revision)\n        if len(repo_files) == 0:\n            raise FileNotFoundError(\n                f\"No local files found for the repo ID {repo_id} and revision {revision}. Please check the repo ID and\"\n                \" the revision or try to download the tokenizer without setting `local_files_only` to `True`.\"\n            )\n\n    valid_tokenizer_files = []\n    tokenizer_file: str\n\n    instruct_versions = list(TokenizerVersion.__members__)\n    mm_versions = list(MultiModalVersion.__members__) + [\"\"]  # allow no mm version\n    sentencepiece_suffixes = [f\".model.{v}{m}\" for v in instruct_versions for m in mm_versions] + [\".model\"]\n\n    for repo_file in repo_files:\n        pathlib_repo_file = Path(repo_file)\n        file_name = pathlib_repo_file.name\n        suffix = \"\".join(pathlib_repo_file.suffixes)\n        if file_name == \"tekken.json\":\n            valid_tokenizer_files.append(file_name)\n        elif suffix in sentencepiece_suffixes:\n            valid_tokenizer_files.append(file_name)\n\n    if len(valid_tokenizer_files) == 0:\n        raise ValueError(f\"No tokenizer file found for model ID: {repo_id}\")\n    # If there are multiple tokenizer files, we use tekken.json if it exists, otherwise the versioned one.\n    if len(valid_tokenizer_files) &gt; 1:\n        if \"tekken.json\" in valid_tokenizer_files:\n            tokenizer_file = \"tekken.json\"\n        else:\n            tokenizer_file = sorted(valid_tokenizer_files)[-1]\n        logger.warning(f\"Multiple tokenizer files found for model ID: {repo_id}. Using {tokenizer_file}.\")\n    else:\n        tokenizer_file = valid_tokenizer_files[0]\n\n    tokenizer_path = huggingface_hub.hf_hub_download(\n        repo_id=repo_id,\n        cache_dir=cache_dir,\n        filename=tokenizer_file,\n        token=token,\n        revision=revision,\n        local_files_only=local_files_only,\n        force_download=force_download,\n    )\n    return tokenizer_path\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.list_local_hf_repo_files","title":"<code>list_local_hf_repo_files(repo_id, revision)</code>","text":"<p>List the files of a local Hugging Face repo.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face repo ID.</p> required <code>revision</code> <code>Optional[str]</code> <p>The revision of the model to use. If <code>None</code>, the latest revision will be used.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def list_local_hf_repo_files(repo_id: str, revision: Optional[str]) -&gt; list[str]:\n    r\"\"\"List the files of a local Hugging Face repo.\n\n    Args:\n        repo_id: The Hugging Face repo ID.\n        revision: The revision of the model to use. If `None`, the latest revision will be used.\n    \"\"\"\n    _assert_hub_installed()\n\n    repo_cache = Path(huggingface_hub.constants.HF_HUB_CACHE) / huggingface_hub.constants.REPO_ID_SEPARATOR.join(\n        [\"models\", *repo_id.split(\"/\")]\n    )\n\n    if revision is None:\n        revision_file = repo_cache / \"refs\" / huggingface_hub.constants.DEFAULT_REVISION\n        if revision_file.is_file():\n            with revision_file.open(\"r\") as file:\n                revision = file.read()\n\n    if revision:\n        revision_dir = repo_cache / \"snapshots\" / revision\n        if revision_dir.is_dir():\n            return os.listdir(revision_dir)\n\n    return []\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>We have a few examples of how to use the library with our models:</p> <ul> <li>Use Codestral to handle FIM requests: FIM example. Note that this model is also capable to handle chat completion requests.</li> <li>Use Mistral Small 3.1 to handle image chat completion requests: Chat Completion example</li> </ul>"},{"location":"examples/codestral_fim/","title":"Fill-In-the-Middle (FIM) using Codestral","text":"<p>We will use mistral-inference to run Codestral.</p>"},{"location":"examples/codestral_fim/#setup","title":"Setup","text":"<p>First, install <code>mistral-inference</code> and <code>mistral-common</code>: <pre><code>pip install mistral-inference mistral-common\n</code></pre></p>"},{"location":"examples/codestral_fim/#running-the-model","title":"Running the model","text":""},{"location":"examples/codestral_fim/#download-the-model","title":"Download the model","text":"<p>Download the model from Hugging Face. <pre><code>pip install huggingface_hub[cli]\n\nhuggingface-cli login --token your_hf_token\nhuggingface-cli download mistralai/Codestral-22B-v0.1 --local-dir ~/codestral-22B-240529\n</code></pre></p>"},{"location":"examples/codestral_fim/#perform-the-fim-task","title":"Perform the FIM task","text":"<p>The Codestral tokenizer is the v3 MistralTokenizer from Mistral.</p> <pre><code>from mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.tokens.instruct.request import FIMRequest\n\ntokenizer = MistralTokenizer.v3()\nmodel = Transformer.from_folder(\"~/codestral-22B-240529\")\n\nprefix = \"\"\"def add(\"\"\"\nsuffix = \"\"\"    return sum\"\"\"\n\nrequest = FIMRequest(prompt=prefix, suffix=suffix)\n\ntokens = tokenizer.encode_fim(request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nmiddle = result.split(suffix)[0].strip()\nprint(middle)\n</code></pre> <p>The output should be: <pre><code>\"\"\"num1, num2):\n\n    sum = num1 + num2\"\"\"\n</code></pre></p>"},{"location":"examples/small_3.1_chat_completion/","title":"Chat completion using Mistral Small 3.1","text":"<p>We will use vLLM to run Mistral Small 3.1.</p>"},{"location":"examples/small_3.1_chat_completion/#setup","title":"Setup","text":"<p>First, install <code>vLLM</code>: <pre><code>pip install vllm\n</code></pre></p> <p>You don't need to install <code>mistral-common</code> as vLLM supports directly Mistral models and tokenizers !</p> <p>In case you had already installed <code>mistral-common</code>, you can upgrade it to the latest version: <pre><code>pip install --upgrade mistral-common\n</code></pre></p>"},{"location":"examples/small_3.1_chat_completion/#running-the-model","title":"Running the model","text":"<p>We recommend that you use Mistral-Small-3.1-24B-Instruct-2503 in a server/client setting.</p>"},{"location":"examples/small_3.1_chat_completion/#launch-the-server","title":"Launch the server","text":"<p>To launch the server, run the following command:</p> <pre><code>vllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 2\n</code></pre> <p>As you can see the following arguments need the <code>mistral</code> specific value:</p> <ul> <li><code>--tokenizer_mode</code></li> <li><code>--config_format</code></li> <li><code>--load_format</code></li> <li><code>--tool-call-parser</code></li> </ul> <p>Thanks to that, internally vLLM will know to use the tokenizers and tool-calls that are defined in <code>mistral-common</code>, you can enjoy our full model features natively.</p>"},{"location":"examples/small_3.1_chat_completion/#chat-completion","title":"Chat Completion","text":"<p>Let's use a ChatCompletionRequest so that we can normalize and validate it before sending it to vLLM. Because <code>Mistral Small 3.1 24B Instruct</code> use the v7 tokenizer, we will use the InstructRequestNormalizerV7 to normalize the request.</p> <p>We will also use the system prompt defined in Mistral-Small-3.1-24B-Instruct-2503.</p> <pre><code>from mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    ImageURLChunk,\n    SystemMessage,\n    TextChunk,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.normalize import InstructRequestNormalizerV7\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_common.protocol.instruct.validator import MistralRequestValidatorV5\n\nvalidator = MistralRequestValidatorV5()\nrequest_normalizer = InstructRequestNormalizerV7.normalizer()\n\nsystem_prompt = \"\"\"You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is 03-06-2025.\n\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" =&gt; \"Where are you?\" or \"When is the next flight to Tokyo\" =&gt; \"Where do you travel from?\").\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is 02-06-2025) and when asked about information at specific dates, you discard information that is at another date.\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\nNext sections describe the capabilities that you have.\n\n# WEB BROWSING INSTRUCTIONS\n\nYou cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n\n# MULTI-MODAL INSTRUCTIONS\n\nYou have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\nYou cannot read nor transcribe audio files or videos.\"\"\"\n\n\nmessages = [\n    SystemMessage(content=system_prompt),\n    UserMessage(content=\"What is the capital of France?\"),\n    AssistantMessage(content=\"The capital of France is Paris.\"),\n    UserMessage(content=[TextChunk(text=\"Does this photo comes from there ?\")]),\n    UserMessage(\n        content=[\n            ImageURLChunk(\n                image_url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/1280px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg\"\n            )\n        ]\n    ),\n]\n\nrequest = ChatCompletionRequest(\n    messages=messages,\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n)\nvalidator.validate_request(request) # No error means the request is valid\ninstruct_request = request_normalizer.from_chat_completion_request(request) # Normalize the request and convert it to an InstructRequest\nprint(instruct_request.messages) \n</code></pre> <p>The messages are now normalized and the last two users messages are merged into one and you should see this:</p> <pre><code>[SystemMessage(role='system', content='You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\nYou power an AI assistant called Le Chat.\\nYour knowledge base was last updated on 2023-10-01.\\nThe current date is 03-06-2025.\\n\\nWhen you\\'re not sure about some information, you say that you don\\'t have the information and don\\'t make up anything.\\nIf the user\\'s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" =&gt; \"Where are you?\" or \"When is the next flight to Tokyo\" =&gt; \"Where do you travel from?\").\\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is 02-06-2025) and when asked about information at specific dates, you discard information that is at another date.\\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\\nNext sections describe the capabilities that you have.\\n\\n# WEB BROWSING INSTRUCTIONS\\n\\nYou cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\\n\\n# MULTI-MODAL INSTRUCTIONS\\n\\nYou have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\\nYou cannot read nor transcribe audio files or videos.'),\n  UserMessage(role='user', content='What is the capital of France?'),\n  AssistantMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None, prefix=False),\n  UserMessage(role='user', content=[TextChunk(type='text', text='Does this photo comes from there ?'), ImageURLChunk(type='image_url', image_url='https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/1280px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg')])]\n</code></pre> <p>Now let's convert the request to something vLLM can understand:</p> <pre><code>vllm_request = instruct_request.to_openai(\n    temperature=0.15,\n    model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n)\n</code></pre> <p>Finally, let's send the request to vLLM:</p> <pre><code>import json\n\nimport requests\n\nurl = \"http://&lt;your-url&gt;:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(vllm_request))\nassistant_response_content = response.json()[\"choices\"][0][\"message\"][\"content\"]\nprint(assistant_response_content)\n</code></pre> <p>Your response should look like this: <pre><code>Yes, this photo is of Paris, France. The Eiffel Tower is prominently visible in the center of the image, which is an iconic landmark of Paris. The river in the foreground is the Seine, and the cityscape includes many of the characteristic buildings and architecture found in Paris.\n</code></pre></p> <p>Now let's try to use the <code>get_weather</code> tool:</p> <pre><code>from mistral_common.protocol.instruct.tool_calls import ToolCall\n\ndef get_current_weather(location, format):\n    if \"Paris\" in location:\n        return {\"temperature\": \"20\", \"unit\": format, \"description\": \"sunny\"}\n    else:\n        return {\"temperature\": \"10\", \"unit\": format, \"description\": \"rainy\"}\n\nnew_request = instruct_request.model_copy(deep=True)\n\nnew_request.messages.append(AssistantMessage(content=assistant_response_content))\nnew_request.messages.append(UserMessage(content=\"Could you tell me what is the weather there ?\"))\n\nnew_vllm_request = new_request.to_openai(\n    temperature=0.15,\n    model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n)\n\nresponse = requests.post(url, headers=headers, data=json.dumps(new_vllm_request))\ntool_call_content = response.json()[\"choices\"][0][\"message\"][\"tool_calls\"]\n\ntool_call = ToolCall(\n    **tool_call_content[0]\n)\n\nassert tool_call.function.name == \"get_current_weather\"\n\nprint(\n    get_current_weather(\n        **json.loads(tool_call.function.arguments)\n    )\n)\n</code></pre> <p>You should see that the model correctly called the tool to give you the weather in Paris:</p> <pre><code>{'temperature': '20', 'unit': 'celsius', 'description': 'sunny'}\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#installation","title":"Installation","text":"<p>See installation instructions.</p>"},{"location":"usage/#getting-started","title":"Getting Started","text":"<p><code>mistral-common</code> is a collection of common utilities for Mistral to construct valid requests and call tokenizers.</p> <p>To take full advantage of the features of our models, you will need to use text, images, and tools. Building a valid request can be challenging, so we've created a set of tools to help you get started. </p> <p>See the following sections for more details:</p> <ul> <li>The requests and their different types.</li> <li>The images.</li> <li>The tools.</li> <li>The tokenizers and their different layer of abstractions that we use.</li> </ul>"},{"location":"usage/images/","title":"Images","text":"<p>Most of the recently released Mistral models support image inputs. Images are represented as BaseContentChunk objects within the <code>messages</code> field of the ChatCompletionRequest. Encoding an image via a ImageEncoder will return:</p> <ul> <li>a sequence of special tokens representing the image.</li> <li>the image normalized as a numpy array.</li> </ul>"},{"location":"usage/images/#supported-image-formats","title":"Supported image formats","text":"<p>Mistral Image encoders use Pillow to decode images and OpenCV to encode. Hence, the supported formats are the same as Pillow's. The images can be provided as: - an ImageURLChunk: a pydantic model containing an image URL from which the image will be downloaded. - an ImageChunk: a pydantic model containing a serialized image that can be either a base64 string or a pillow image.</p>"},{"location":"usage/images/#use-an-image-encoder-with-our-tokenizer","title":"Use an Image encoder with our tokenizer","text":"<p>Our tokenizers can an ImageEncoder that is configured with ImageConfig.</p> <p>The attributes of the ImageConfig configure how the images will be patched into tokens:</p> <ul> <li><code>image_patch_size</code>: the square size of a patch in pixels to form one token. E.g if the image is 224x224 and the patch size is 14, then the image will be divided into 16x16 patches.</li> <li><code>max_image_size</code>: the maximum size of the image in pixels. If the image is larger, it will be resized to this size.</li> <li><code>spatial_merge_size</code>: the number of patches to merge into one token. This is useful to reduce the number of redundant tokens in the image. E.g if the image is 224x224 and the patch size is 14, then the image will be divided into 16x16 patches. If the spatial merge size is 2, then the image will be divided into 8x8 patches.</li> </ul> <pre><code>from mistral_common.protocol.instruct.messages import ImageURLChunk\nfrom mistral_common.tokens.tokenizers.image import ImageEncoder, ImageConfig, SpecialImageIDs\n\nspecial_ids = SpecialImageIDs(img=10, img_break=11, img_end=12)  # These are normally automatically set by the tokenizer\n\nconfig = ImageConfig(image_patch_size=14, max_image_size=224, spatial_merge_size=2)\n\nimage = ImageURLChunk(image_url=\"https://live.staticflickr.com/7250/7534338696_b33e941b7d_b.jpg\")\n\nencoder = ImageEncoder(config, special_ids)\nencoder(image)\n</code></pre>"},{"location":"usage/images/#tokenize-an-image","title":"Tokenize an image","text":"<p>Let's load the tekken tokenizer used for Mistral Small 3.1's tokenizer to encode and tokenize an image.</p> <pre><code>from huggingface_hub import hf_hub_download\n\nfrom mistral_common.protocol.instruct.messages import ImageURLChunk, TextChunk, UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nmodel_id = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\ntokenizer = MistralTokenizer.from_hf_hub(repo_id=model_id, token=\"your_hf_token\")\n\ntokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        messages=[\n            UserMessage(\n                content=[\n                    ImageURLChunk(image_url=\"https://live.staticflickr.com/7250/7534338696_b33e941b7d_b.jpg\"),\n                    TextChunk(text=\"What is displayed in this image?\"),\n                ]\n            )\n        ],\n    )\n)\n# Tokenized(tokens=[1, 3, 10, 10, ...], text='&lt;s&gt;[INST][IMG][IMG][IMG][IMG]...', prefix_ids=None, images=[array[[(0.95238595, 0.95238795, 0.95224484, ...,)]]])\n</code></pre> <p>The output contains:</p> <ul> <li>the text: the string equivalent of the tokens. The image is represented into a sequence of special <code>[IMG]</code> tokens with <code>[IMG_BREAK]</code> at regular intervals and <code>[IMG_END]</code> at the end. An image is a grid and each <code>[IMG]</code> represent a patch, the <code>IMG_BREAK</code> tokens the end of a row. The <code>IMG_END</code> token is used to mark the end of the image.</li> <li>the tokens: identifier used by the model for the text. The special tokens of images are not directly used by the model, but replaced by the features of an image encoder.</li> <li>the prefix_ids: Used for FIM (Fill-In-the-Middle) tasks, here it is <code>None</code>.</li> <li>the images: the images normalized as a numpy array.</li> </ul>"},{"location":"usage/install/","title":"Install","text":""},{"location":"usage/install/#pip","title":"Pip","text":"<p>You can install the library using pip: <pre><code>pip install mistral-common[opencv]\n</code></pre></p>"},{"location":"usage/install/#from-source","title":"From source","text":"<p>To build it for source, you can clone the repository and install it using uv or pip. We recommend using uv for faster and more reliable dependency resolution: <pre><code>git clone https://github.com/mistralai/mistral-common.git\ncd mistral-common\nuv sync --frozen\nuv pip install . # or `uv pip install -e .` for development\n</code></pre></p>"},{"location":"usage/requests/","title":"Requests","text":"<p>To query an AI assistant like Mistral's LeChat or ChatGPT you need to provide the following:</p> <ul> <li>The history of the conversation between the user, the assistant and the tool calls.</li> <li>The tools available to the assistant.</li> <li>The context of the request.</li> </ul> <p>In <code>mistral-common</code>, we currently support the following requests types:</p> <ul> <li>Instruct requests:<ul> <li>Chat completion requests.</li> <li>Fill-In-the-Middle completion.</li> </ul> </li> <li>Embedding requests.</li> </ul> <p>Every instruct requests should be encoded with it's corresponding <code>encode_function</code> function by the tokenizers.</p>"},{"location":"usage/requests/#chat-completion","title":"Chat completion","text":"<p>Chat completion consists in a conversation between a user and an assistant. The assistant can call tools to enrich its response. Some of our tokenizers also support the use of images (see the images)</p> <p>Every chat completion request are defined via ChatCompletionRequest.</p> <p>To perform actual task every requests should follow the following structure:</p> <ol> <li>validate the request via a MistralRequestValidator</li> <li>normalize the requests via the InstructRequestNormalizer.</li> <li>encode the request.</li> </ol> <p>Using the MistralTokenizer.encode_chat_completion method will perform all these steps for you.</p> <p>Following this design ensures minimizing unexpected behavior from the user.</p>"},{"location":"usage/requests/#conversation","title":"Conversation","text":"<p>A conversation with a model is a sequence of messages. Each message can be a user message, an assistant message, a tool message or a system message. To ease the creation of these messages, we provide a set of Pydantic classes that you can use to create them:</p> <ul> <li>UserMessage: a message from the user. Users are the ones that interact with the model.</li> <li>AssistantMessage: a message from the assistant. The assistant is the model itself.</li> <li>ToolMessage: a message from a tool. Tools are functions that the model can call to get information to answer the user's question.</li> <li>SystemMessage: a message from the system. Also called <code>System Prompt</code>, it is a set of instructions that the model should follow to answer the user's question. This allows you to customize the behavior of the model.</li> </ul>"},{"location":"usage/requests/#tools","title":"Tools","text":"<p>Tools are functions that the model can call to get information to answer the user's question. See the Tools section for more information.</p>"},{"location":"usage/requests/#example","title":"Example","text":"<p>Here is an example of a request where a user asks for the weather in Paris. The model is also given access to a <code>get_current_weather</code> tool to get the weather:</p> <pre><code>from mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nrequest = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris\"),\n    ],\n)\n\ntokenizer = MistralTokenizer.v3()\ntokenizer.encode_chat_completion(request)\n</code></pre>"},{"location":"usage/requests/#fim","title":"FIM","text":"<p>Fill In the Middle (FIM) is a task where the model is given a prefix and a suffix and is asked to fill in the middle. This is useful for code completion, where the model is given a prefix of code and is asked to complete the code.</p> <p>A pydantic class FIMRequest is defined to ease the creation of these requests.</p> <pre><code>from mistral_common.tokens.instruct.request import FIMRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nrequest = FIMRequest(\n    prompt=\"def hello_world():\\n    print('Hello, world!')\",\n    suffix=\"\\n\\nhello_world()\",\n)\n\ntokenizer = MistralTokenizer.v3()\ntokenizer.encode_fim(request)\n</code></pre>"},{"location":"usage/requests/#embedding","title":"Embedding","text":"<p>Embedding is a task where the model is given a text and is asked to return a vector representation of the text. This is useful for semantic search, where you want to find texts that are similar to a given text.</p> <p>A pydantic class EmbeddingRequest is defined to ease the creation of these requests.</p> <pre><code>from mistral_common.protocol.embedding.request import EmbeddingRequest\n\nrequest = EmbeddingRequest(\n    model=\"mistral-small-2409\",\n    input=\"Hello, world!\",\n)\n</code></pre>"},{"location":"usage/tokenizers/","title":"Tokenizers","text":""},{"location":"usage/tokenizers/#how-do-we-perform-tokenization","title":"How do we perform tokenization ?","text":"<p>Tokenization is the process of converting a sequence of characters into a sequence of tokens. To dive deeper into the tokenization process, you can read the tokenization guide on our website.</p> <p>Historically, we based our tokenizers on the following libraries:</p> <ul> <li>SentencePiece: we moved from this library to the one below.  </li> <li>tiktoken: we use this library for our tekken tokenizers. It is more efficient for multilingual tokenization.</li> </ul>"},{"location":"usage/tokenizers/#tokenizer","title":"Tokenizer","text":"<p>For our releases, we provide a tokenizer file that you can use to load a tokenizer. If the tokenizer is based on the <code>tiktoken</code> library, the file is generally named <code>tekken.json</code>. Here is how you could load the Mistral Small 3.1 Instruct's tokenizer:</p> <pre><code>from mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\ntokenizer = MistralTokenizer.from_hf_hub(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", token=\"your_hf_token\")\n\nchat_completion = ChatCompletionRequest(\n    messages=[\n        UserMessage(role=\"user\", content=\"Hello, how are you?\"),\n    ]\n)\ntokenizer.encode_chat_completion(chat_completion).tokens\n# [1, 3, 22177, 1044, 2606, 1584, 1636, 1063, 4]\n</code></pre> <p>We provide three layer of abstraction for our tokenizers:</p> <ul> <li>Tekkenizer or SentencePieceTokenizer: raw tokenizers to handle raw data to convert them into ids and vice versa.</li> <li>InstructTokenizer: instruct tokenizers that wrap the raw tokenizers to add several helper methods for the different tasks (chat completion or FIM). They are versioned.</li> <li>MistralTokenizer: mistral tokenizers that validate the requests, see requests section, and call the instruct tokenizers.</li> </ul> <p>For instance, you can directly load the Tekkenizer:</p> <pre><code>from huggingface_hub import hf_hub_download\n\nfrom mistral_common.tokens.tokenizers.tekken  import Tekkenizer\n\nmodel_id = \"mistralai/Devstral-Small-2505\"\ntekken_file = hf_hub_download(repo_id=model_id, filename=\"tekken.json\", token=\"your_hf_token\")\n\ntokenizer = Tekkenizer.from_file(tekken_file)\ntokenizer.encode(\"Plz tekkenize me\", bos=True, eos=True)\n# [1, 4444, 1122, 16058, 3569, 2033, 1639, 2]\n</code></pre> <p>See the Examples section for examples on how to use the tokenizers with our models.</p>"},{"location":"usage/tokenizers/#special-tokens","title":"Special tokens","text":"<p>Special tokens are tokens that have a special meaning for the model. They are used to mark the beginning or end of a sequence, an image, tools, etc. For example, in the Mistral Small 3.1 Instruct tokenizer, some special tokens include:</p> <ul> <li><code>&lt;s&gt;</code>: Beginning of a sequence</li> <li><code>&lt;/s&gt;</code>: End of a sequence</li> <li><code>[INST]</code>: Beginning of an instruction</li> <li><code>[/INST]</code>: End of an instruction</li> <li><code>[TOOL_CALLS]</code>: Beginning of a tool call</li> <li><code>[IMG]</code>: Content of an image</li> <li><code>[IMG_BREAK]</code>: End of a row in an image</li> <li><code>[IMG_END]</code>: End of an image</li> <li>...</li> </ul> <p>These tokens are defined in the tokenizer configuration file (recommended) or at instantiation for the Tekkenizer (deprecated).</p> <p>In <code>mistral-common</code>, special tokens are never encoded directly. This means that:</p> <pre><code>tokenizer.encode(\"&lt;s&gt;\")\n</code></pre> <p>will not return the ID of the <code>&lt;s&gt;</code> token. Instead, it will return a list of IDs corresponding to the tokenization of the string <code>\"&lt;s&gt;\"</code>. The special token IDs are added directly to the sequence of IDs when encoding the requests.</p> <p>To add new tokens to the tokenizer and use them correctly, you need to:</p> <ol> <li>Add the tokens to the tokenizer configuration file.</li> <li>Include the special token IDs in the input IDs when encoding requests.</li> </ol> <p>We are open to suggestions for improvement. Please open an issue if you have any feedback.</p>"},{"location":"usage/tools/","title":"Tools","text":"<p>To execute external tools, like searching the web, LLMs need to be given the ability to call such tools whenever they see fit. This is usually achieved by adding all available tools that the LLM can call into the context of the ChatCompletionRequest and representing tool calls as well as tool message as its own objects.</p>"},{"location":"usage/tools/#define-tools","title":"Define tools","text":"<p>To define tools in <code>mistral-common</code>, you use the Tool class. Let's define a simple tool that fetches the current weather for a given city.</p> <p>The example function below is not a real call that calls the weather, but it very well be an call to a weather API. We hardcode random numbers here just for demonstrative purposes.</p> <pre><code>from mistral_common.protocol.instruct.tool_calls import Tool, Function\n\n\ndef get_current_weather(location: str, format: str) -&gt; int:\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return 22 if format == \"celsius\" else 72\n    elif \"san francisco\" in location.lower():\n        return 14 if format == \"celsius\" else 57\n    elif \"paris\" in location.lower():\n        return 18 if format == \"celsius\" else 64\n    else:\n        return 20 if format == \"celsius\" else 68\n\nweather_tool = Tool(\n    function=Function(\n        name=\"get_current_weather\",\n        description=\"Get the current weather\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"format\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                },\n            },\n            \"required\": [\"location\", \"format\"],\n        },\n    )\n)\n</code></pre>"},{"location":"usage/tools/#tool-calling-and-messages","title":"Tool calling and messages","text":"<p>When a model decides to call a tool, it will add a ToolCall to the AssistantMessage. The tool call will have an <code>id</code> and a FunctionCall with the name of the function and the arguments to pass to it.</p> <pre><code>from mistral_common.protocol.instruct.messages import AssistantMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import (\n    Function,\n    FunctionCall,\n    Tool,\n    ToolCall,\n)\n\n\nAssistantMessage(\n    content=None,\n    tool_calls=[\n        ToolCall(\n            id=\"VvvODy9mT\",\n            function=FunctionCall(\n                name=\"get_current_weather\",\n                arguments='{\"location\": \"Paris, France\", \"format\": \"celsius\"}',\n            ),\n        )\n    ],\n)\n</code></pre> <p>Then you can execute the function and return the result in a ToolMessage. The ToolMessage must have the same <code>id</code> as the ToolCall it's responding to.</p> <pre><code>from mistral_common.protocol.instruct.messages import ToolMessage\n\n\nToolMessage(tool_call_id=\"VvvODy9mT\", name=\"get_current_weather\", content=\"22\")\n</code></pre>"},{"location":"usage/tools/#putting-it-all-together","title":"Putting it all together","text":"<p>Now we can put it all together and create a ChatCompletionRequest with the tools and the messages. We can then use the MistralTokenizer to encode the request.</p> <pre><code>from mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    SystemMessage,\n    ToolMessage,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import (\n    Function,\n    FunctionCall,\n    Tool,\n    ToolCall,\n)\n\nrequest = ChatCompletionRequest(\n    messages=[\n        SystemMessage(content=\"You are a helpful assistant.\"),\n        UserMessage(content=\"What's the weather like in Paris?\"),\n        AssistantMessage(\n            content=None,\n            tool_calls=[\n                ToolCall(\n                    id=\"VvvODy9mT\",\n                    function=FunctionCall(\n                        name=\"get_current_weather\",\n                        arguments='{\"location\": \"Paris, France\", \"format\": \"celsius\"}',\n                    ),\n                )\n            ],\n        ),\n        ToolMessage(tool_call_id=\"VvvODy9mT\", name=\"get_current_weather\", content=\"22\"),\n    ],\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n)\n\ntokenizer = MistralTokenizer.v3()\ntokens = tokenizer.encode_chat_completion(request).tokens\n</code></pre> <p>Now the model can use the tool result to answer the user's question.</p>"}]}